{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0865aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import zlib\n",
    "import base64\n",
    "import requests\n",
    "import operator\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# sqlite-vec expects the standard library sqlite3 module.\n",
    "# Older versions of this notebook replaced sqlite3 with sqlean via sys.modules; undo that if present.\n",
    "if sys.modules.get(\"sqlite3\") is sys.modules.get(\"sqlean\"):\n",
    "    del sys.modules[\"sqlite3\"]\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Annotated, List, TypedDict, Optional, Dict, Any, Tuple, Literal\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import SQLiteVec\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from prompts import DECOMPOSER_SYSTEM, GENERATOR_SYSTEM, CRITIC_SYSTEM, REFLECTOR_SYSTEM, PLAN_AUDITOR_SYSTEM\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e7d7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeNames(str, Enum):\n",
    "    \"\"\"Enum for node names to avoid string literals.\"\"\"\n",
    "    RETRIEVE = \"retrieve\"\n",
    "    DECOMPOSE = \"decompose\"\n",
    "    GENERATE = \"generate\"\n",
    "    SYNTAX_CHECK = \"syntax_check\"\n",
    "    CRITIC = \"critic\"\n",
    "    REFLECTOR = \"reflector\"\n",
    "    PLAN_AUDIT = \"plan_audit\"\n",
    "\n",
    "\n",
    "class Scores(float, Enum):\n",
    "    \"\"\"Score thresholds for validation.\"\"\"\n",
    "    AVERAGE_SCORE_THRESHOLD = 7.5\n",
    "    REQUIREMENT_COVERAGE_THRESHOLD = 8.0\n",
    "\n",
    "\n",
    "class Attribute(BaseModel):\n",
    "    \"\"\"Model for a class attribute.\"\"\"\n",
    "    name: str = Field(description=\"Attribute name\")\n",
    "    type: str = Field(default=\"\", description=\"Attribute type (e.g., String, int)\")\n",
    "\n",
    "\n",
    "class Class(BaseModel):\n",
    "    \"\"\"Model for a UML class.\"\"\"\n",
    "    name: str = Field(description=\"Class name\")\n",
    "    attributes: List[Attribute] = Field(default_factory=list, description=\"List of class attributes\")\n",
    "\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Model for a relationship between classes.\"\"\"\n",
    "    source: str = Field(description=\"Source class name\")\n",
    "    target: str = Field(description=\"Target class name\")\n",
    "    type: str = Field(description=\"Relationship type (e.g., association, composition, inheritance)\")\n",
    "    multiplicity: Optional[str] = Field(default=None, description=\"Multiplicity (e.g., '1..*', '0..1')\")\n",
    "\n",
    "\n",
    "class DecompositionResult(BaseModel):\n",
    "    \"\"\"Structured output from the DECOMPOSE node.\"\"\"\n",
    "    classes: List[Class] = Field(default_factory=list, description=\"List of identified classes\")\n",
    "    relationships: List[Relationship] = Field(default_factory=list, description=\"List of relationships between classes\")\n",
    "\n",
    "\n",
    "class CritiqueFinding(BaseModel):\n",
    "    \"\"\"Model for a single finding from the critic.\"\"\"\n",
    "    description: str = Field(description=\"Description of the finding\")\n",
    "    severity: Literal[\"error\", \"warning\", \"info\"] = Field(default=\"warning\", description=\"Severity level\")\n",
    "    fixability: Literal[\"render_only\", \"requires_redesign\", \"unfixable\"] = Field(\n",
    "        default=\"render_only\", \n",
    "        description=\"Whether the issue can be fixed by rendering, requires redesign, or is unfixable\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CritiqueError(BaseModel):\n",
    "    \"\"\"Model for a single critique error.\"\"\"\n",
    "    type: str = Field(description=\"Type of error\")\n",
    "    description: str = Field(description=\"Detailed description of the error\")\n",
    "\n",
    "\n",
    "class CritiqueResponse(BaseModel):\n",
    "    \"\"\"Structured output from the CRITIC node.\"\"\"\n",
    "    requirement_coverage: float = Field(ge=0, le=10, description=\"Does it capture all classes and relationships from the text?\")\n",
    "    design_best_practices: float = Field(ge=0, le=10, description=\"Are relationships correct? (e.g., composition vs association)\")\n",
    "    structural_integrity: float = Field(ge=0, le=10, description=\"Are there redundant classes or missing attributes?\")\n",
    "    is_valid: bool = Field(default=False, description=\"True only if total average score is > threshold AND 'requirement_coverage' is >= threshold\")\n",
    "    errors: List[CritiqueError] = Field(default_factory=list, description=\"List of errors found\")\n",
    "    warnings: List[str] = Field(default_factory=list, description=\"List of warnings\")\n",
    "    missing_concepts: List[str] = Field(default_factory=list, description=\"Concepts from requirements not in diagram\")\n",
    "    findings: List[CritiqueFinding] = Field(default_factory=list, description=\"Structured findings for reflector\")\n",
    "    reasoning: str = Field(description=\"Brief explanation for the scores provided.\")\n",
    "\n",
    "    @property\n",
    "    def weighted_score(self) -> float:\n",
    "        return (self.requirement_coverage * 0.5) + \\\n",
    "               (self.design_best_practices * 0.3) + \\\n",
    "               (self.structural_integrity * 0.2)\n",
    "    \n",
    "    @model_validator(mode='after')\n",
    "    def compute_validity(self) -> 'CritiqueResponse':\n",
    "        self.is_valid = (self.weighted_score > Scores.AVERAGE_SCORE_THRESHOLD) and (self.requirement_coverage >= Scores.REQUIREMENT_COVERAGE_THRESHOLD)\n",
    "        return self\n",
    "\n",
    "\n",
    "class SummaryResponse(BaseModel):\n",
    "    \"\"\"Structured output from the SUMMARIZER node.\"\"\"\n",
    "    is_complete: bool = Field(description=\"Whether all issues are resolved\")\n",
    "    fixed: List[str] = Field(default_factory=list, description=\"Issues that were fixed\")\n",
    "    unresolved: List[str] = Field(default_factory=list, description=\"Issues still present\")\n",
    "    message: str = Field(description=\"Brief status summary\")\n",
    "\n",
    "\n",
    "class PlanAudit(BaseModel):\n",
    "    is_valid: bool = Field(description=\"True if the plan is logically sound and covers all requirements.\")\n",
    "    critique: List[str] = Field(default_factory=list, description=\"List of specific logical flaws (e.g., 'Missing relationship between User and Account').\")\n",
    "    suggestions: List[str] = Field(default_factory=list, description=\"Actionable steps to fix the plan.\")\n",
    "    \n",
    "    @model_validator(mode='after')\n",
    "    def validate_consistency(self) -> 'PlanAudit':\n",
    "        \"\"\"Ensure is_valid is consistent with critique content.\n",
    "        \n",
    "        If the LLM says invalid but provides no critique, we treat it as valid\n",
    "        since there are no actual issues identified.\n",
    "        \"\"\"\n",
    "        if not self.is_valid and not self.critique:\n",
    "            self.is_valid = True\n",
    "            logger.info(\"Plan audit: LLM returned is_valid=False with empty critique, overriding to valid\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class SystemConfig(BaseModel):\n",
    "    \"\"\"System configuration for UML generation.\"\"\"\n",
    "    lmstudio_base_url: str = Field(default=\"http://localhost:1234/v1\", description=\"LMStudio API endpoint\")\n",
    "    model_name: str = Field(default=\"mistralai/devstral-small-2-2512\", description=\"Model to use\")\n",
    "    embedder_model: str = Field(default=\"BAAI/bge-large-en-v1.5\", description=\"Embedder model for semantic search\")\n",
    "    db_path: str = Field(default=\"./../data/uml_knowledge.db\", description=\"Path to SQLite database\")\n",
    "    shots_json_path: str = Field(default=\"./../data/complete_shots.json\", description=\"Path to few-shot examples\")\n",
    "    plantuml_host: str = Field(default=\"http://localhost:8080\", description=\"PlantUML server host\")\n",
    "    max_iterations: int = Field(default=6, ge=1, description=\"Maximum workflow iterations\")\n",
    "    max_tokens_decompose: int = Field(default=1024, description=\"Max tokens for decompose step\")\n",
    "    max_tokens_generate: int = Field(default=2048, description=\"Max tokens for generate step\")\n",
    "    max_tokens_critique: int = Field(default=2048, description=\"Max tokens for critique step\")\n",
    "    max_tokens_summarize: int = Field(default=1024, description=\"Max tokens for summarize step\")\n",
    "    max_tokens_reflect: int = Field(default=2048, description=\"Max tokens for reflect step\")\n",
    "    max_tokens_compare: int = Field(default=1024, description=\"Max tokens for compare step\")\n",
    "    temperature: float = Field(default=0.15, ge=0.0, le=2.0, description=\"Base temperature for LLM\")\n",
    "    num_few_shots: int = Field(default=3, ge=0, description=\"Number of few-shot examples\")\n",
    "    request_timeout: int = Field(default=5, ge=1, description=\"Timeout for PlantUML server requests\")\n",
    "    llm_timeout: int = Field(default=120, ge=1, description=\"Timeout for LLM operations\")\n",
    "    # New fields for iteration metrics\n",
    "    plateau_window: int = Field(default=3, ge=2, description=\"Number of iterations to consider for plateau detection\")\n",
    "    plateau_threshold: float = Field(default=0.1, ge=0.0, description=\"Score delta threshold for plateau detection\")\n",
    "    max_stagnant_iterations: int = Field(default=2, ge=1, description=\"Max consecutive stagnant iterations before stopping\")\n",
    "\n",
    "\n",
    "class PlantUMLResult(BaseModel):\n",
    "    \"\"\"Result from PlantUML validation.\"\"\"\n",
    "    is_valid: bool = Field(description=\"Whether the PlantUML syntax is valid\")\n",
    "    error: Optional[str] = Field(default=None, description=\"Error message if validation failed\")\n",
    "    url: Optional[str] = Field(default=None, description=\"URL to view the diagram\")\n",
    "    svg_url: Optional[str] = Field(default=None, description=\"URL to view the diagram as SVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97779ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Shared state for the LangGraph workflow.\n",
    "    \"\"\"\n",
    "    requirements: str\n",
    "    plan: Optional[str]\n",
    "    examples: List[Dict[str, str]]\n",
    "    current_diagram: Optional[str]\n",
    "    best_diagram: Optional[str]  \n",
    "    history: Annotated[List[Dict[str, Any]], operator.add]\n",
    "    summary: Optional[str]\n",
    "    syntax_valid: bool\n",
    "    logic_valid: bool\n",
    "    error_message: Optional[str]\n",
    "    plan_valid: bool \n",
    "    audit_feedback: Optional[List[str]]  # Feedback from auditor\n",
    "    plan_audit_attempts: int  # audit loop iterations\n",
    "    best_score: float\n",
    "    best_code: str\n",
    "    current_validation: Optional[CritiqueResponse]\n",
    "    failed_attempts: Annotated[List[Dict[str, Any]], operator.add]\n",
    "    iterations: int\n",
    "    stagnant_count: int  # Track consecutive iterations without changes\n",
    "    critique_cache: Dict[str, Dict[str, Any]]  # Cache critiques by diagram hash\n",
    "    score_history: List[float]  # History of weighted scores for plateau detection\n",
    "    delta_score: float  # Score change from previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b32b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm(config: Optional[SystemConfig] = None) -> ChatOpenAI:\n",
    "    \"\"\"\n",
    "    Create a ChatOpenAI instance configured for LMStudio.\n",
    "    \n",
    "    Args:\n",
    "        config: Optional system configuration\n",
    "        \n",
    "    Returns:\n",
    "        Configured ChatOpenAI instance\n",
    "    \"\"\"\n",
    "    cfg = config or SystemConfig()\n",
    "    logger.info(f\"Connecting to LMStudio at {cfg.lmstudio_base_url}\")\n",
    "    logger.info(f\"Using model: {cfg.model_name} (temp={cfg.temperature})\")\n",
    "    \n",
    "    return ChatOpenAI(\n",
    "        base_url=cfg.lmstudio_base_url,\n",
    "        api_key=\"lm-studio\",  \n",
    "        model=cfg.model_name,\n",
    "        temperature=cfg.temperature,\n",
    "        timeout=cfg.llm_timeout \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "56dcf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantUMLTool:\n",
    "    \"\"\"\n",
    "    Tool for validating and rendering PlantUML diagrams.\n",
    "    \n",
    "    This class interfaces with a PlantUML server to check syntax\n",
    "    and generate diagram URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"http://localhost:8080\"):\n",
    "        \"\"\"\n",
    "        Initialize PlantUML tool.\n",
    "        \n",
    "        Args:\n",
    "            host: PlantUML server host URL\n",
    "        \"\"\"\n",
    "        self.host = host\n",
    "        logger.info(f\"PlantUML tool initialized with host: {host}\")\n",
    "\n",
    "    def extract_plantuml(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract PlantUML code from markdown blocks or raw text.\n",
    "        \n",
    "        Args:\n",
    "            text: Text containing PlantUML code\n",
    "            \n",
    "        Returns:\n",
    "            Extracted PlantUML code or empty string\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Try to extract from ```plantuml ... ```\n",
    "        fence_match = re.search(r\"```\\s*plantuml\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if fence_match:\n",
    "            return fence_match.group(1).strip()\n",
    "        \n",
    "        # Try to extract from @startuml ... @enduml\n",
    "        tag_match = re.search(r\"@startuml.*?@enduml\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if tag_match:\n",
    "            return tag_match.group(0).strip()\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def _encode_plantuml(self, plantuml_code: str) -> str:\n",
    "        \"\"\"\n",
    "        Encode PlantUML code for URL.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: Raw PlantUML code\n",
    "            \n",
    "        Returns:\n",
    "            URL-safe encoded string\n",
    "        \"\"\"\n",
    "        code = plantuml_code.strip()\n",
    "        \n",
    "        if not code.startswith(\"@startuml\"): \n",
    "            code = f\"@startuml\\n{code}\"\n",
    "        if not code.endswith(\"@enduml\"): \n",
    "            code = f\"{code}\\n@enduml\"\n",
    "        \n",
    "        compressed = zlib.compress(code.encode('utf-8'))[2:-4]\n",
    "        encoded = base64.b64encode(compressed).translate(\n",
    "            bytes.maketrans(\n",
    "                b\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\",\n",
    "                b\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_\"\n",
    "            )\n",
    "        ).decode('utf-8')\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "    def get_diagram_url(self, plantuml_code: str, format: str = \"png\") -> str:\n",
    "        \"\"\"\n",
    "        Generate a viewable URL for the PlantUML diagram.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: PlantUML diagram code\n",
    "            format: Output format (png, svg, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            URL to view the diagram\n",
    "        \"\"\"\n",
    "        diagram_code = self.extract_plantuml(plantuml_code)\n",
    "        encoded = self._encode_plantuml(diagram_code)\n",
    "        return f\"{self.host}/{format}/{encoded}\"\n",
    "        \n",
    "    def check_syntax(self, plantuml_code: str, timeout: int = 5) -> PlantUMLResult:\n",
    "        \"\"\"\n",
    "        Validate PlantUML syntax with detailed error extraction.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: PlantUML code to validate\n",
    "            timeout: Request timeout in seconds\n",
    "            \n",
    "        Returns:\n",
    "            PlantUMLResult with validation status and detailed error if applicable.\n",
    "        \"\"\"\n",
    "        logger.info(\"Validating PlantUML syntax\")\n",
    "        \n",
    "        try:\n",
    "            diagram_code = self.extract_plantuml(plantuml_code)\n",
    "            encoded = self._encode_plantuml(diagram_code)\n",
    "            \n",
    "            url_png = f\"{self.host}/png/{encoded}\"\n",
    "            response = requests.get(url_png, timeout=timeout)\n",
    "            \n",
    "            if response.status_code == 200 and response.content[:4] == b'\\x89PNG':\n",
    "                logger.info(\"Syntax validation passed (PNG rendered)\")\n",
    "                return PlantUMLResult(\n",
    "                    is_valid=True,\n",
    "                    url=url_png,\n",
    "                    svg_url=f\"{self.host}/svg/{encoded}\"\n",
    "                )\n",
    "            \n",
    "            logger.warning(\"PNG rendering failed. Fetching detailed syntax error...\")\n",
    "            url_txt = f\"{self.host}/txt/{encoded}\"\n",
    "            error_response = requests.get(url_txt, timeout=timeout)\n",
    "            \n",
    "            detailed_error = error_response.text.strip() if error_response.status_code == 200 else \"Unknown server error\"\n",
    "            \n",
    "            error_msg = f\"PlantUML Syntax Error:\\n{detailed_error[:1000]}\"\n",
    "            logger.error(f\"Syntax error detected: {error_msg}\")\n",
    "            \n",
    "            return PlantUMLResult(\n",
    "                is_valid=False,\n",
    "                error=error_msg\n",
    "            )\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            error_msg = f\"PlantUML Server Connection Error: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return PlantUMLResult(is_valid=False, error=error_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error during syntax check: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return PlantUMLResult(is_valid=False, error=error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d633d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryManager:\n",
    "    \"\"\"\n",
    "    Manages long-term memory for UML diagram generation using LangChain's SQLiteVec.\n",
    "    \n",
    "    Supports semantic search to find similar past solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder: SentenceTransformer,\n",
    "        db_path: str = \"./../data/uml_knowledge.db\",\n",
    "        embedding_dims: int = 1024\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize memory manager with LangChain SQLiteVec.\n",
    "        \n",
    "        Args:\n",
    "            embedder: SentenceTransformer model for semantic search\n",
    "            db_path: Path to the SQLite database file\n",
    "            embedding_dims: Dimensions of the embeddings \n",
    "        \"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.db_path = db_path\n",
    "        self.embedding_dims = embedding_dims\n",
    "        \n",
    "\n",
    "        self.embedding_function = HuggingFaceEmbeddings(\n",
    "            model_name=embedder.model_name if hasattr(embedder, 'model_name') else \"BAAI/bge-large-en-v1.5\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Create directory and connection\n",
    "        os.makedirs(os.path.dirname(self.db_path) if os.path.dirname(self.db_path) else \".\", exist_ok=True)\n",
    "        \n",
    "        # Create connection using sqlean instead of default sqlite3 (which lacks extension support on macOS)\n",
    "        try:\n",
    "            import sqlean\n",
    "            import sqlite_vec\n",
    "            \n",
    "            self.connection = sqlean.connect(self.db_path)\n",
    "            self.connection.row_factory = sqlean.Row\n",
    "            self.connection.enable_load_extension(True)\n",
    "            sqlite_vec.load(self.connection)\n",
    "            self.connection.enable_load_extension(False)\n",
    "            logger.info(\"Used sqlean for SQLite connection (extension support enabled)\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"sqlean not found, falling back to SQLiteVec.create_connection (may fail on macOS)\")\n",
    "            self.connection = SQLiteVec.create_connection(db_file=self.db_path)\n",
    "        \n",
    "        # Initialize vector store with connection\n",
    "        self.vector_store = SQLiteVec(\n",
    "            table=\"uml_memories\",\n",
    "            connection=self.connection,\n",
    "            embedding=self.embedding_function\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"MemoryManager initialized with LangChain SQLiteVec at {db_path} (dims={embedding_dims})\")\n",
    "\n",
    "    def save_diagram(\n",
    "        self,\n",
    "        requirements: str,\n",
    "        diagram: str,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Save a validated diagram to SQLite long-term memory.\n",
    "                \n",
    "        Args:\n",
    "            requirements: Original requirements text\n",
    "            diagram: PlantUML diagram code\n",
    "            metadata: Optional metadata\n",
    "            \n",
    "        Returns:\n",
    "            ID of the stored record\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        full_metadata = metadata or {}\n",
    "        full_metadata.update({\n",
    "            \"diagram\": diagram,\n",
    "            \"timestamp\": timestamp\n",
    "        })\n",
    "        \n",
    "\n",
    "        doc = Document(\n",
    "            page_content=requirements,\n",
    "            metadata=full_metadata\n",
    "        )\n",
    "        \n",
    "        ids = self.vector_store.add_documents([doc])\n",
    "        \n",
    "        logger.info(\"Diagram saved to SQLite memory using LangChain SQLiteVec\")\n",
    "        return ids[0] if ids else 0\n",
    "    \n",
    "    def retrieve_similar_diagrams(\n",
    "        self,\n",
    "        requirements: str,\n",
    "        limit: int = 2\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve similar diagrams from SQLite memory using vector search.\n",
    "        \n",
    "        Args:\n",
    "            requirements: Requirements text to search for\n",
    "            limit: Maximum number of results\n",
    "            \n",
    "        Returns:\n",
    "            List of similar diagram records\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.vector_store.similarity_search(requirements, k=limit)\n",
    "            \n",
    "            diagrams = []\n",
    "            for doc in results:\n",
    "                diagrams.append({\n",
    "                    \"requirements\": doc.page_content,\n",
    "                    \"diagram\": doc.metadata.get(\"diagram\", \"\"),\n",
    "                    \"timestamp\": doc.metadata.get(\"timestamp\", \"\"),\n",
    "                    \"metadata\": {k: v for k, v in doc.metadata.items() \n",
    "                                if k not in [\"diagram\", \"timestamp\"]}\n",
    "                })\n",
    "                \n",
    "            logger.info(f\"Retrieved {len(diagrams)} similar diagrams from SQLite\")\n",
    "            return diagrams\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Memory retrieval failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def clear_memory(self) -> None:\n",
    "        try:\n",
    "            # Close existing connection\n",
    "            if hasattr(self, 'connection'):\n",
    "                self.connection.close()\n",
    "            \n",
    "            # Remove database file\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            \n",
    "            # Recreate connection and vector store\n",
    "            import sqlean\n",
    "            import sqlite_vec\n",
    "            \n",
    "            self.connection = sqlean.connect(self.db_path)\n",
    "            self.connection.row_factory = sqlean.Row\n",
    "            self.connection.enable_load_extension(True)\n",
    "            sqlite_vec.load(self.connection)\n",
    "            self.connection.enable_load_extension(False)\n",
    "            \n",
    "            self.vector_store = SQLiteVec(\n",
    "                table=\"uml_memories\",\n",
    "                connection=self.connection,\n",
    "                embedding=self.embedding_function\n",
    "            )\n",
    "\n",
    "            logger.info(\"Memory cleared and reinitialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to clear memory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "156ecfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_memory_from_shots(\n",
    "    memory_manager: MemoryManager,\n",
    "    shots_json_path: str = \"./../data/complete_shots.json\",\n",
    "    force_reseed: bool = False\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Seed the memory database with few-shot examples from JSON file.\n",
    "    Skips seeding if database already contains data (unless force_reseed=True).\n",
    "    \n",
    "    Args:\n",
    "        memory_manager: MemoryManager instance to seed\n",
    "        shots_json_path: Path to the complete_shots.json file\n",
    "        force_reseed: If True, clears existing data and reseeds\n",
    "        \n",
    "    Returns:\n",
    "        Number of shots seeded (0 if skipped)\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"CHECKING MEMORY SEEDING STATUS\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Check if database already has data\n",
    "    try:\n",
    "        existing_docs = memory_manager.vector_store.similarity_search(\"test\", k=1)\n",
    "        if existing_docs and not force_reseed:\n",
    "            logger.info(f\"Database already contains data ({len(existing_docs)} docs found)\")\n",
    "            logger.info(\"Skipping seeding operation. Set force_reseed=True to override.\")\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Database appears empty or uninitialized: {e}\")\n",
    "    \n",
    "    if force_reseed:\n",
    "        logger.warning(\"Force reseed enabled - clearing existing memory\")\n",
    "        memory_manager.clear_memory()\n",
    "    \n",
    "\n",
    "    if not os.path.exists(shots_json_path):\n",
    "        logger.error(f\"Shots file not found at {shots_json_path}\")\n",
    "        return 0\n",
    "    \n",
    "    logger.info(f\"Loading shots from {shots_json_path}\")\n",
    "    with open(shots_json_path, 'r', encoding='utf-8') as f:\n",
    "        shots = json.load(f)\n",
    "    \n",
    "    logger.info(f\"Found {len(shots)} shots to seed\")\n",
    "    \n",
    "    # Prepare documents\n",
    "    documents = []\n",
    "    for shot in shots:\n",
    "        requirements = shot[\"requirements\"]\n",
    "        diagram = shot[\"solution_plantuml\"]\n",
    "        \n",
    "        metadata = {\n",
    "            \"diagram\": diagram,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"plan\": shot.get(\"subgoal_decomposition\"),\n",
    "            \"reasoning\": shot.get(\"chain_of_thought\"),\n",
    "            \"is_static\": True,\n",
    "            \"title\": shot.get(\"title\", \"Untitled\")\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"  Processing: {metadata['title']}\")\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=requirements,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    if documents:\n",
    "        memory_manager.vector_store.add_documents(documents)\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"✓ Successfully seeded {len(documents)} shots to memory\")\n",
    "        logger.info(\"=\"*60)\n",
    "        return len(documents)\n",
    "    \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UMLNodes:\n",
    "    \"\"\"\n",
    "    Collection of agent nodes for the UML generation workflow.\n",
    "    \n",
    "    Each method represents a node in the LangGraph workflow and\n",
    "    follows the pattern of taking AgentState and returning a dict\n",
    "    with state updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        plantuml_tool: PlantUMLTool,\n",
    "        memory_manager: Optional['MemoryManager'] = None,\n",
    "        config: Optional[SystemConfig] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize UML nodes with required dependencies.\n",
    "        \n",
    "        Args:\n",
    "            llm: LangChain ChatOpenAI instance\n",
    "            plantuml_tool: Tool for PlantUML validation\n",
    "            memory_manager: long-term memory manager\n",
    "            config: Optional system configuration\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.plantuml_tool = plantuml_tool\n",
    "        self.memory_manager = memory_manager\n",
    "        self.config = config or SystemConfig()\n",
    "        logger.info(\"UMLNodes initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_diagram(diagram: str) -> str:\n",
    "        \"\"\"Normalize diagram for consistent hashing (remove whitespace variations).\"\"\"\n",
    "        lines = [line.strip() for line in diagram.strip().split('\\n') if line.strip()]\n",
    "        return '\\n'.join(sorted(lines))  # Sort for order-independent comparison\n",
    "    \n",
    "    @staticmethod\n",
    "    def _hash_diagram(diagram: str) -> str:\n",
    "        \"\"\"Create a hash of the diagram content for caching.\"\"\"\n",
    "        normalized = UMLNodes._normalize_diagram(diagram)\n",
    "        return hashlib.md5(normalized.encode()).hexdigest()\n",
    "\n",
    "    def _safe_invoke(self, runnable: Any, input_data: Any, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Invoke a runnable (LLM or chain) with retry logic.\n",
    "        \"\"\"\n",
    "        max_retries = 3\n",
    "        last_exception = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return runnable.invoke(input_data, **kwargs)\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                logger.warning(f\"LLM call failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 * (attempt + 1))\n",
    "        \n",
    "        logger.error(f\"Max retries reached for LLM call: {last_exception}\")\n",
    "        raise last_exception\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_decomposition_plan(decomposition: DecompositionResult) -> str:\n",
    "        \"\"\"\n",
    "        Format a DecompositionResult into a readable plan string.\n",
    "        \n",
    "        Args:\n",
    "            decomposition: The decomposition result with classes and relationships\n",
    "            \n",
    "        Returns:\n",
    "            Formatted plan as a string\n",
    "        \"\"\"\n",
    "        lines = [\"## STRUCTURAL DECOMPOSITION\\n\"]\n",
    "        \n",
    "        # Format classes\n",
    "        if decomposition.classes:\n",
    "            lines.append(\"### Classes:\")\n",
    "            for cls in decomposition.classes:\n",
    "                attrs_str = \", \".join([f\"{attr.name}: {attr.type}\" for attr in cls.attributes])\n",
    "                lines.append(f\"- {cls.name}\" + (f\" ({attrs_str})\" if attrs_str else \"\"))\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Format relationships\n",
    "        if decomposition.relationships:\n",
    "            lines.append(\"### Relationships:\")\n",
    "            for rel in decomposition.relationships:\n",
    "                multiplicity_str = f\" [{rel.multiplicity}]\" if rel.multiplicity else \"\"\n",
    "                lines.append(f\"- {rel.source} --{rel.type}--> {rel.target}{multiplicity_str}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def retrieve(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant few-shot examples based on requirements.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'examples' key containing formatted shots\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.RETRIEVE.upper()} ---\")\n",
    "        \n",
    "        try:\n",
    "            memories = self.memory_manager.retrieve_similar_diagrams(\n",
    "                state[\"requirements\"],\n",
    "                limit=self.config.num_few_shots\n",
    "            )\n",
    "            \n",
    "            formatted_shots = []\n",
    "            for mem in memories:\n",
    "                formatted_shots.append(\n",
    "                    HumanMessage(content=f\"Requirements:\\n{mem['requirements']}\")\n",
    "                )\n",
    "                \n",
    "                meta = mem.get(\"metadata\", {})\n",
    "                plan = meta.get(\"plan\", \"No plan available.\")\n",
    "                reasoning = meta.get(\"reasoning\", \"No reasoning available.\")\n",
    "                \n",
    "                assistant_content = (\n",
    "                    f\"1. DESIGN PLAN:\\n{plan}\\n\\n\"\n",
    "                    f\"2. DESIGN REASONING:\\n{reasoning}\\n\\n\"\n",
    "                    f\"3. PLANTUML DIAGRAM:\\n```plantuml\\n{mem['diagram']}\\n```\"\n",
    "                )\n",
    "                \n",
    "                formatted_shots.append(\n",
    "                    AIMessage(content=assistant_content)\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(memories)} relevant examples from unified memory\")\n",
    "            return {\"examples\": formatted_shots}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Retrieval failed: {e}\")\n",
    "            return {\"examples\": []}\n",
    "\n",
    "    def decompose(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Decompose requirements into structural building blocks.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'plan' update containing formatted decomposition\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.DECOMPOSE.upper()} ---\")\n",
    "\n",
    "        feedback = state.get(\"audit_feedback\", [])\n",
    "        feedback_str = \"\\n\".join([f\"- {f}\" for f in feedback]) if feedback else \"None\"\n",
    "\n",
    "        system_prompt = DECOMPOSER_SYSTEM\n",
    "        if feedback:\n",
    "            system_prompt += f\"\\n\\nIMPORTANT: Your previous plan was rejected. Fix these issues:\\n{feedback_str}\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=f\"REQUIREMENTS:\\n{state['requirements']}\")\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            structured_llm = self.llm.bind(max_tokens=self.config.max_tokens_decompose).with_structured_output(DecompositionResult)\n",
    "            decomposition: DecompositionResult = self._safe_invoke(\n",
    "                structured_llm,\n",
    "                messages\n",
    "            )\n",
    "            logger.info(\"Decomposition completed\")\n",
    "            formatted_plan = self._format_decomposition_plan(decomposition)\n",
    "            \n",
    "            return {\"plan\": formatted_plan}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Decomposition failed: {e}\")\n",
    "            empty_decomposition = DecompositionResult(classes=[], relationships=[])\n",
    "            formatted_plan = self._format_decomposition_plan(empty_decomposition)\n",
    "            return {\"plan\": formatted_plan}\n",
    "        \n",
    "    def plan_auditor(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Audits the structural plan for logical consistency and requirement coverage.\n",
    "        \n",
    "        Uses PlanAudit Pydantic model for structured output validation.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'plan_valid' and 'audit_feedback' updates\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.PLAN_AUDIT.upper()} ---\")\n",
    "        \n",
    "        # Check if we've exceeded max plan audit attempts\n",
    "        max_plan_audit_attempts = 3\n",
    "        current_attempts = state.get(\"plan_audit_attempts\", 0) + 1\n",
    "        \n",
    "        if current_attempts > max_plan_audit_attempts:\n",
    "            logger.warning(f\"Max plan audit attempts ({max_plan_audit_attempts}) reached. Forcing plan validation.\")\n",
    "            return {\n",
    "                \"plan_valid\": True,\n",
    "                \"audit_feedback\": [],\n",
    "                \"plan_audit_attempts\": current_attempts\n",
    "            }\n",
    "        \n",
    "        plan = state.get(\"plan\")\n",
    "        requirements = state.get(\"requirements\")\n",
    "        req_len = len(requirements) if isinstance(requirements, str) else 0\n",
    "        plan_len = len(plan) if isinstance(plan, str) else 0\n",
    "        logger.debug(f\"Plan audit inputs: requirements_len={req_len}, plan_len={plan_len}\")\n",
    "        logger.debug(f\"Plan audit prompt template chars={len(PLAN_AUDITOR_SYSTEM)}\")\n",
    "        if not requirements:\n",
    "            logger.warning(\"Plan audit received empty requirements\")\n",
    "        if not plan:\n",
    "            logger.warning(\"Plan audit received empty plan\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        REQUIREMENTS:\n",
    "        {requirements}\n",
    "        \n",
    "        PROPOSED PLAN:\n",
    "        {plan}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            messages = [\n",
    "                SystemMessage(content=PLAN_AUDITOR_SYSTEM),\n",
    "                HumanMessage(content=prompt)\n",
    "            ]\n",
    "            # Questa sezione di codice si può probabilmente semplificare\n",
    "            try:\n",
    "                structured_llm = self.llm.with_structured_output(PlanAudit, include_raw=True)\n",
    "                audit_payload = structured_llm.invoke(messages)\n",
    "                audit_result: Optional[PlanAudit] = audit_payload.get(\"parsed\")\n",
    "                raw_msg = audit_payload.get(\"raw\")\n",
    "                parsing_error = audit_payload.get(\"parsing_error\")\n",
    "                if raw_msg is not None and getattr(raw_msg, 'content', None) is not None:\n",
    "                    raw_text = str(raw_msg.content)\n",
    "                    raw_preview = raw_text[:2000] + (\"...\" if len(raw_text) > 2000 else \"\")\n",
    "                    logger.debug(f\"Plan audit raw response (preview): {raw_preview}\")\n",
    "                if parsing_error is not None:\n",
    "                    logger.warning(f\"Plan audit parsing_error: {parsing_error}\")\n",
    "                if audit_result is None:\n",
    "                    raise ValueError(\"Plan audit produced no parsed result\")\n",
    "            except TypeError:\n",
    "                audit_result = self.llm.with_structured_output(PlanAudit).invoke(messages)\n",
    "            elapsed = time.perf_counter() - t0\n",
    "            logger.debug(f\"Plan audit invoke took {elapsed:.2f}s\")\n",
    "            \n",
    "            logger.info(f\"Plan audit completed: valid={audit_result.is_valid}\")\n",
    "            logger.debug(f\"Plan audit parsed: critique_count={len(audit_result.critique)}, suggestions_count={len(audit_result.suggestions)}\")\n",
    "            if not audit_result.is_valid:\n",
    "                if audit_result.critique:\n",
    "                    logger.info(f\"Audit issues (first 5): {', '.join(audit_result.critique[:5])}\")\n",
    "                if audit_result.suggestions:\n",
    "                    logger.info(f\"Audit suggestions (first 5): {', '.join(audit_result.suggestions[:5])}\")\n",
    "            logger.debug(f\"Plan audit full result: {audit_result.model_dump()}\")\n",
    "            \n",
    "            return {\n",
    "                \"plan_valid\": audit_result.is_valid,\n",
    "                \"audit_feedback\": audit_result.critique,\n",
    "                \"plan_audit_attempts\": current_attempts\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Plan audit failed: {e}\")\n",
    "            return {\n",
    "                \"plan_valid\": False,\n",
    "                \"audit_feedback\": [f\"Audit error: {str(e)}\"],\n",
    "                \"plan_audit_attempts\": current_attempts  \n",
    "            }\n",
    "\n",
    "    def generate(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate PlantUML diagram using chain-of-thought reasoning.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'current_diagram' and 'iterations' updates\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.GENERATE.upper()} ---\")\n",
    "        \n",
    "        messages = [SystemMessage(content=GENERATOR_SYSTEM)]\n",
    "        \n",
    "        # Add few-shot examples if available\n",
    "        if state.get(\"examples\"):\n",
    "            messages.extend(state[\"examples\"])\n",
    "            logger.debug(f\"Added {len(state['examples'])} example messages\")\n",
    "            \n",
    "        user_content = f\"\"\"\n",
    "        # VALIDATED REQUIREMENTS\n",
    "        {state['requirements']}\n",
    "\n",
    "        # VALIDATED DESIGN PLAN\n",
    "        {state['plan']}\n",
    "\n",
    "        # TASK\n",
    "        Render the PlantUML class diagram exactly from the design plan.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages.append(HumanMessage(content=user_content))\n",
    "        \n",
    "        try:\n",
    "            response = self._safe_invoke(\n",
    "                self.llm,\n",
    "                messages,\n",
    "                max_tokens=self.config.max_tokens_generate\n",
    "            )\n",
    "            diagram = self.plantuml_tool.extract_plantuml(response.content)\n",
    "            \n",
    "            logger.info(f\"Generation completed (iteration {state['iterations'] + 1})\")\n",
    "            return {\n",
    "                \"current_diagram\": diagram,\n",
    "                \"iterations\": state[\"iterations\"] + 1,\n",
    "                \"stagnant_count\": 0  # Reset stagnation counter on new generation\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "            return {\n",
    "                \"current_diagram\": f\"Error: {str(e)}\",\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }\n",
    "\n",
    "    def syntax_check(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate PlantUML syntax through server.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'syntax_valid' and optional 'error_message'\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.SYNTAX_CHECK.upper()} ---\")\n",
    "        \n",
    "        try:\n",
    "            result = self.plantuml_tool.check_syntax(\n",
    "                state[\"current_diagram\"],\n",
    "                timeout=self.config.request_timeout\n",
    "            )\n",
    "            \n",
    "            if result.is_valid:\n",
    "                logger.info(f\"Syntax valid. View at: {result.url}\")\n",
    "            else:\n",
    "                logger.warning(f\"Syntax error: {result.error}\")\n",
    "            \n",
    "            return {\n",
    "                \"syntax_valid\": result.is_valid,\n",
    "                \"error_message\": result.error if not result.is_valid else None,\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Syntax check failed: {e}\")\n",
    "            return {\n",
    "                \"syntax_valid\": False,\n",
    "                \"error_message\": f\"Syntax check error: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    def _validate_diagram(\n",
    "        self, \n",
    "        requirements: str, \n",
    "        diagram: str, \n",
    "        cache: Optional[Dict[str, Dict[str, Any]]] = None\n",
    "    ) -> Tuple[CritiqueResponse, bool]:\n",
    "        \"\"\"\n",
    "        Helper method to validate a diagram and return the structured response.\n",
    "        Uses caching to avoid re-evaluating identical diagrams.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (CritiqueResponse, was_cached: bool)\n",
    "        \"\"\"\n",
    "        diagram_hash = self._hash_diagram(diagram)\n",
    "        \n",
    "        if cache and diagram_hash in cache:\n",
    "            cached = cache[diagram_hash]\n",
    "            logger.info(f\"Using cached critique for diagram (hash: {diagram_hash[:8]}...)\")\n",
    "            return CritiqueResponse(\n",
    "                requirement_coverage=cached[\"requirement_coverage\"],\n",
    "                design_best_practices=cached[\"design_best_practices\"],\n",
    "                structural_integrity=cached[\"structural_integrity\"],\n",
    "                is_valid=cached[\"is_valid\"],\n",
    "                errors=[CritiqueError(**e) for e in cached.get(\"errors\", [])],\n",
    "                warnings=cached.get(\"warnings\", []),\n",
    "                missing_concepts=cached.get(\"missing_concepts\", []),\n",
    "                reasoning=cached.get(\"reasoning\", \"Cached result\")\n",
    "            ), True\n",
    "        \n",
    "        # Not cached, perform actual evaluation\n",
    "        plantuml_only = self.plantuml_tool.extract_plantuml(diagram)\n",
    "\n",
    "        user_msg = f\"\"\"\n",
    "        # REQUIREMENTS\n",
    "        {requirements}\n",
    "\n",
    "        # DIAGRAM\n",
    "        {plantuml_only}\n",
    "\n",
    "        Evaluate the diagram strictly according to the audit rules.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=CRITIC_SYSTEM),\n",
    "            HumanMessage(content=user_msg)\n",
    "        ]\n",
    "        \n",
    "        structured_llm = self.llm.with_structured_output(CritiqueResponse)\n",
    "        result = self._safe_invoke(structured_llm, messages)\n",
    "        \n",
    "        return result, False\n",
    "\n",
    "    def critic(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform logical validation of the UML diagram.\n",
    "        Uses caching to avoid re-evaluating identical diagrams.\n",
    "                \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'logic_valid' and 'history' updates\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.CRITIC.upper()} ---\")\n",
    "        \n",
    "        cache = state.get(\"critique_cache\", {})\n",
    "        \n",
    "        try:\n",
    "            critique_response, was_cached = self._validate_diagram(\n",
    "                requirements=state[\"requirements\"],\n",
    "                diagram=state[\"current_diagram\"],\n",
    "                cache=cache\n",
    "            )\n",
    "            \n",
    "            weighted = critique_response.weighted_score\n",
    "            is_valid = critique_response.is_valid\n",
    "            \n",
    "            critique = {\n",
    "                \"is_valid\": critique_response.is_valid,\n",
    "                \"requirement_coverage\": critique_response.requirement_coverage,\n",
    "                \"design_best_practices\": critique_response.design_best_practices,\n",
    "                \"structural_integrity\": critique_response.structural_integrity,\n",
    "                \"weighted_score\": weighted,\n",
    "                \"errors\": [{\"type\": err.type, \"description\": err.description} \n",
    "                          for err in critique_response.errors],\n",
    "                \"warnings\": critique_response.warnings,\n",
    "                \"missing_concepts\": critique_response.missing_concepts,\n",
    "                \"reasoning\": critique_response.reasoning\n",
    "            }\n",
    "            \n",
    "            if not was_cached:\n",
    "                diagram_hash = self._hash_diagram(state[\"current_diagram\"])\n",
    "                cache[diagram_hash] = critique\n",
    "            \n",
    "            is_valid = critique_response.is_valid\n",
    "            cache_status = \"(CACHED)\" if was_cached else \"(NEW)\"\n",
    "            logger.info(f\"Logic validation {cache_status}: {'PASSED' if is_valid else 'FAILED'} (Weighted Score: {weighted:.2f})\")\n",
    "            logger.info(f\"  Requirements Coverage: {critique_response.requirement_coverage:.2f}/10\")\n",
    "            logger.info(f\"  Design Best Practices: {critique_response.design_best_practices:.2f}/10\")\n",
    "            logger.info(f\"  Structural Integrity: {critique_response.structural_integrity:.2f}/10\")\n",
    "            \n",
    "            if not is_valid and critique_response.errors:\n",
    "                logger.info(f\"Found {len(critique_response.errors)} errors\")\n",
    "            \n",
    "            updates = {\n",
    "                \"logic_valid\": is_valid,\n",
    "                \"history\": [critique] + state.get(\"history\", []),\n",
    "                \"current_validation\": critique_response,\n",
    "                \"critique_cache\": cache\n",
    "            }\n",
    "            \n",
    "            if is_valid and not state.get(\"best_diagram\"):\n",
    "                logger.info(f\"Storing first valid diagram as best\")\n",
    "                updates[\"best_diagram\"] = state[\"current_diagram\"]\n",
    "            \n",
    "            return updates\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critic failed: {e}\")\n",
    "            return {\n",
    "                \"logic_valid\": False,\n",
    "                \"history\": [{\n",
    "                    \"is_valid\": False,\n",
    "                    \"requirement_coverage\": 0.0,\n",
    "                    \"design_best_practices\": 0.0,\n",
    "                    \"structural_integrity\": 0.0,\n",
    "                    \"weighted_score\": 0.0,\n",
    "                    \"errors\": [{\"type\": \"system\", \"description\": str(e)}],\n",
    "                    \"warnings\": [],\n",
    "                    \"missing_concepts\": [],\n",
    "                    \"reasoning\": \"System error during critique.\"\n",
    "                }],\n",
    "                \"critique_cache\": cache\n",
    "            }\n",
    "\n",
    "    def reflector(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Reflector node: fixes render-only issues in the current PlantUML diagram\n",
    "        based on structured critique findings.\n",
    "\n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "\n",
    "        Returns:\n",
    "            Dict with updated current_diagram, history, and iterations\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.REFLECTOR.upper()} ---\")\n",
    "\n",
    "        try:\n",
    "            # Extract findings flagged as render-only from current_validation\n",
    "            current_validation = state.get(\"current_validation\")\n",
    "            findings = current_validation.findings if current_validation else []\n",
    "            renderable_findings = [\n",
    "                f for f in findings if f.fixability == \"render_only\"\n",
    "            ]\n",
    "\n",
    "            # If no render-only findings, also check for errors to provide context\n",
    "            if not renderable_findings and current_validation:\n",
    "                error_context = []\n",
    "                for err in current_validation.errors:\n",
    "                    error_context.append({\n",
    "                        \"description\": err.description,\n",
    "                        \"severity\": \"error\",\n",
    "                        \"fixability\": \"render_only\"\n",
    "                    })\n",
    "                for warn in current_validation.warnings:\n",
    "                    error_context.append({\n",
    "                        \"description\": warn,\n",
    "                        \"severity\": \"warning\",\n",
    "                        \"fixability\": \"render_only\"\n",
    "                    })\n",
    "                \n",
    "                if not error_context:\n",
    "                    logger.info(\"No issues to fix. Returning current diagram unchanged.\")\n",
    "                    return {\n",
    "                        \"current_diagram\": state[\"current_diagram\"],\n",
    "                        \"iterations\": state[\"iterations\"] + 1\n",
    "                    }\n",
    "                \n",
    "                reflector_input = {\n",
    "                    \"diagram\": state[\"current_diagram\"],\n",
    "                    \"findings\": error_context\n",
    "                }\n",
    "            else:\n",
    "                reflector_input = {\n",
    "                    \"diagram\": state[\"current_diagram\"],\n",
    "                    \"findings\": [f.model_dump() for f in renderable_findings]\n",
    "                }\n",
    "\n",
    "            system_msg = SystemMessage(content=REFLECTOR_SYSTEM)\n",
    "            user_msg = HumanMessage(content=json.dumps(reflector_input))\n",
    "\n",
    "            reflected_diagram_response = self._safe_invoke(\n",
    "                self.llm,\n",
    "                [system_msg, user_msg],\n",
    "                max_tokens=self.config.max_tokens_reflect\n",
    "            )\n",
    "\n",
    "            reflected_diagram = self.plantuml_tool.extract_plantuml(reflected_diagram_response.content)\n",
    "\n",
    "            logger.info(f\"Reflector applied fixes for {len(reflector_input['findings'])} issues.\")\n",
    "\n",
    "            history_entry = {\n",
    "                \"node\": \"reflector\",\n",
    "                \"original_diagram\": state[\"current_diagram\"],\n",
    "                \"reflected_diagram\": reflected_diagram,\n",
    "                \"fixed_findings\": [f[\"description\"] for f in reflector_input[\"findings\"]]\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"current_diagram\": reflected_diagram,\n",
    "                \"history\": [history_entry],\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reflector node failed: {e}\")\n",
    "            return {\n",
    "                \"current_diagram\": state.get(\"current_diagram\"),\n",
    "                \"error_message\": str(e),\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d3bf9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_iteration_metrics(state: AgentState, cfg: SystemConfig) -> None:\n",
    "    \"\"\"\n",
    "    Update iteration-based metrics for stopping conditions:\n",
    "    - Weighted score delta\n",
    "    - Plateau detection\n",
    "    - Stagnant iteration count\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state\n",
    "        cfg: System configuration (plateau_window, plateau_threshold)\n",
    "    \"\"\"\n",
    "    current_validation = state.get(\"current_validation\")\n",
    "    if not current_validation:\n",
    "        state[\"delta_score\"] = 0.0\n",
    "        state[\"stagnant_count\"] = state.get(\"stagnant_count\", 0)\n",
    "        return\n",
    "\n",
    "    current_score = getattr(current_validation, \"weighted_score\", 0.0)\n",
    "    state.setdefault(\"score_history\", []).insert(0, current_score)  # newest first\n",
    "\n",
    "    # Limit history to plateau window\n",
    "    state[\"score_history\"] = state[\"score_history\"][:cfg.plateau_window]\n",
    "\n",
    "    # Compute delta from previous iteration\n",
    "    if len(state[\"score_history\"]) > 1:\n",
    "        delta = current_score - state[\"score_history\"][1]\n",
    "    else:\n",
    "        delta = current_score  # first iteration\n",
    "\n",
    "    state[\"delta_score\"] = delta\n",
    "\n",
    "    # Check for plateau\n",
    "    plateau_detected = False\n",
    "    if len(state[\"score_history\"]) >= cfg.plateau_window:\n",
    "        deltas = [\n",
    "            abs(state[\"score_history\"][i] - state[\"score_history\"][i+1])\n",
    "            for i in range(len(state[\"score_history\"]) - 1)\n",
    "        ]\n",
    "        if all(d < cfg.plateau_threshold for d in deltas):\n",
    "            plateau_detected = True\n",
    "\n",
    "    # Update stagnant count\n",
    "    if plateau_detected:\n",
    "        state[\"stagnant_count\"] = state.get(\"stagnant_count\", 0) + 1\n",
    "    else:\n",
    "        state[\"stagnant_count\"] = 0\n",
    "\n",
    "    logger.debug(\n",
    "        f\"Iteration metrics updated: current_score={current_score:.2f}, \"\n",
    "        f\"delta_score={delta:.2f}, stagnant_count={state['stagnant_count']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "12124ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uml_graph(\n",
    "    nodes: UMLNodes, \n",
    "    config: Optional[SystemConfig] = None\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Create the LangGraph workflow for UML diagram generation.\n",
    "    \n",
    "    Args:\n",
    "        nodes: UMLNodes instance with all agent methods\n",
    "        config: Optional system configuration\n",
    "        \n",
    "    Returns:\n",
    "        Compiled LangGraph workflow\n",
    "    \"\"\"\n",
    "    cfg = config or SystemConfig()\n",
    "    logger.info(\"Creating UML generation workflow\")\n",
    "    \n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Add all nodes\n",
    "    workflow.add_node(NodeNames.RETRIEVE, nodes.retrieve)\n",
    "    workflow.add_node(NodeNames.PLAN_AUDIT, nodes.plan_auditor)\n",
    "    workflow.add_node(NodeNames.DECOMPOSE, nodes.decompose)\n",
    "    workflow.add_node(NodeNames.GENERATE, nodes.generate)\n",
    "    workflow.add_node(NodeNames.SYNTAX_CHECK, nodes.syntax_check)\n",
    "    workflow.add_node(NodeNames.CRITIC, nodes.critic)\n",
    "    workflow.add_node(NodeNames.REFLECTOR, nodes.reflector)\n",
    "    \n",
    "    logger.debug(\"Added 7 nodes to workflow\")\n",
    "\n",
    "    # Define edges\n",
    "    workflow.add_edge(START, NodeNames.RETRIEVE)\n",
    "    workflow.add_edge(NodeNames.RETRIEVE, NodeNames.DECOMPOSE)\n",
    "    workflow.add_edge(NodeNames.DECOMPOSE, NodeNames.PLAN_AUDIT)\n",
    "\n",
    "    def route_after_plan_audit(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Route based on plan audit results.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Next node name\n",
    "        \"\"\"\n",
    "        if state[\"plan_valid\"]:\n",
    "            logger.debug(\"Routing: plan_audit -> generate\")\n",
    "            return NodeNames.GENERATE\n",
    "            \n",
    "        logger.debug(\"Routing: plan_audit -> decompose\")\n",
    "        return NodeNames.DECOMPOSE\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.PLAN_AUDIT, \n",
    "        route_after_plan_audit,\n",
    "        {\n",
    "            NodeNames.DECOMPOSE: NodeNames.DECOMPOSE,\n",
    "            NodeNames.GENERATE: NodeNames.GENERATE\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(NodeNames.GENERATE, NodeNames.SYNTAX_CHECK)\n",
    "\n",
    "    def route_after_syntax_check(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Route based on syntax validation results and iteration limits.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Next node name\n",
    "        \"\"\"\n",
    "        if state[\"syntax_valid\"]:\n",
    "            logger.debug(\"Routing: syntax_check -> critic\")\n",
    "            return NodeNames.CRITIC\n",
    "            \n",
    "        if state[\"iterations\"] >= cfg.max_iterations:\n",
    "            logger.warning(f\"Max iterations ({cfg.max_iterations}) reached during syntax check\")\n",
    "            return END\n",
    "            \n",
    "        logger.debug(\"Routing: syntax_check -> reflect\")\n",
    "        return NodeNames.REFLECTOR\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.SYNTAX_CHECK, \n",
    "        route_after_syntax_check,\n",
    "        {\n",
    "            NodeNames.CRITIC: NodeNames.CRITIC,\n",
    "            NodeNames.REFLECTOR: NodeNames.REFLECTOR,\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def route_after_critic(state: AgentState) -> str:\n",
    "        update_iteration_metrics(state, cfg)\n",
    "\n",
    "        if state.get(\"logic_valid\"):\n",
    "            return END\n",
    "        \n",
    "        # Check for unfixable findings\n",
    "        current_validation = state.get(\"current_validation\")\n",
    "        if current_validation and current_validation.findings:\n",
    "            if any(f.fixability == \"unfixable\" for f in current_validation.findings):\n",
    "                logger.warning(\"Unfixable issues detected, ending workflow\")\n",
    "                return END\n",
    "        \n",
    "        if state[\"iterations\"] >= cfg.max_iterations:\n",
    "            logger.warning(f\"Max iterations ({cfg.max_iterations}) reached\")\n",
    "            return END\n",
    "        if state.get(\"stagnant_count\", 0) >= cfg.max_stagnant_iterations:\n",
    "            logger.warning(f\"Stagnation detected ({cfg.max_stagnant_iterations} iterations without improvement)\")\n",
    "            return END\n",
    "        return NodeNames.REFLECTOR\n",
    "\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.CRITIC, \n",
    "        route_after_critic,\n",
    "        {\n",
    "            END: END,\n",
    "            NodeNames.REFLECTOR: NodeNames.REFLECTOR\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def route_after_reflector(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Route after reflection based on iteration limits and stagnation.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Next node name\n",
    "        \"\"\"\n",
    "        if state[\"iterations\"] >= cfg.max_iterations:\n",
    "            logger.warning(f\"Max iterations ({cfg.max_iterations}) reached after reflection\")\n",
    "            return END\n",
    "        logger.debug(\"Routing: reflect -> syntax_check\")\n",
    "        return NodeNames.SYNTAX_CHECK\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.REFLECTOR, \n",
    "        route_after_reflector,\n",
    "        {\n",
    "            NodeNames.SYNTAX_CHECK: NodeNames.SYNTAX_CHECK,\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Workflow graph created successfully\")\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "def create_initial_state(requirements: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Create an initial state for the workflow.\n",
    "    \n",
    "    Args:\n",
    "        requirements: Software requirements text\n",
    "        \n",
    "    Returns:\n",
    "        Initial AgentState dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"requirements\": requirements,\n",
    "        \"plan\": None,\n",
    "        \"examples\": [],\n",
    "        \"current_diagram\": None,\n",
    "        \"best_diagram\": None,\n",
    "        \"history\": [],\n",
    "        \"summary\": None,\n",
    "        \"syntax_valid\": False,\n",
    "        \"logic_valid\": False,\n",
    "        \"iterations\": 0,\n",
    "        \"error_message\": None,\n",
    "        \"failed_attempts\": [],\n",
    "        \"stagnant_count\": 0,\n",
    "        \"critique_cache\": {},\n",
    "        # Additional required fields\n",
    "        \"plan_valid\": False,\n",
    "        \"audit_feedback\": None,\n",
    "        \"plan_audit_attempts\": 0,\n",
    "        \"best_score\": 0.0,\n",
    "        \"best_code\": \"\",\n",
    "        \"current_validation\": None,\n",
    "        \"score_history\": [],\n",
    "        \"delta_score\": 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "549e0c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 04:15:01,515 - __main__ - INFO - Loading test exercises from ./../data/test_exercises.json\n",
      "2026-01-15 04:15:01,519 - __main__ - INFO - Loaded 8 test exercises\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 test exercises\n"
     ]
    }
   ],
   "source": [
    "def load_test_exercises(json_path: str = \"./../data/test_exercises.json\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load test exercises from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to test exercises JSON\n",
    "        \n",
    "    Returns:\n",
    "        List of exercise dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If file doesn't exist\n",
    "        json.JSONDecodeError: If JSON is invalid\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading test exercises from {json_path}\")\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"Test exercises file not found: {json_path}\")\n",
    "    \n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        exercises = json.load(f)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(exercises)} test exercises\")\n",
    "    return exercises\n",
    "\n",
    "try:\n",
    "    test_exercises = load_test_exercises()\n",
    "    print(f\"Loaded {len(test_exercises)} test exercises\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load test exercises: {e}\")\n",
    "    test_exercises = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e53188a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 04:15:01,532 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:01,533 - __main__ - INFO - INITIALIZING UML GENERATION SYSTEM\n",
      "2026-01-15 04:15:01,534 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:01,535 - __main__ - INFO - Creating LLM connection...\n",
      "2026-01-15 04:15:01,536 - __main__ - INFO - Connecting to LMStudio at http://localhost:1234/v1\n",
      "2026-01-15 04:15:01,536 - __main__ - INFO - Using model: mistralai/devstral-small-2-2512 (temp=0.15)\n",
      "2026-01-15 04:15:01,537 - __main__ - INFO - Initializing PlantUML tool...\n",
      "2026-01-15 04:15:01,538 - __main__ - INFO - PlantUML tool initialized with host: http://localhost:8080\n",
      "2026-01-15 04:15:01,538 - __main__ - INFO - Initializing long-term memory with BAAI/bge-large-en-v1.5...\n",
      "2026-01-15 04:15:01,541 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps\n",
      "2026-01-15 04:15:01,542 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "2026-01-15 04:15:01,996 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:02,079 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:02,284 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:02,362 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:02,536 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:02,604 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:02,797 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:02,879 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:03,069 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:03,159 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:03,341 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:03,423 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:03,596 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-15 04:15:03,797 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:03,885 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec83dd9298d243838c549d8c9731d0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-15 04:15:04,306 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:04,382 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:04,579 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-15 04:15:04,759 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:05,026 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:05,119 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:05,327 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5 \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:35,668 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "2026-01-15 04:15:36,003 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:36,065 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:36,237 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:36,302 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:36,490 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:36,562 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:36,760 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:36,823 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:37,004 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:37,079 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:37,263 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:37,322 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:37,491 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-15 04:15:37,672 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:37,735 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912bb786c45b474495b4181f4521e1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-15 04:15:38,276 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:38,356 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:38,557 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-15 04:15:38,750 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:39,058 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-15 04:15:39,128 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:39,375 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5 \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:15:39,604 - __main__ - INFO - Used sqlean for SQLite connection (extension support enabled)\n",
      "2026-01-15 04:15:43,103 - __main__ - INFO - MemoryManager initialized with LangChain SQLiteVec at ./../data/uml_knowledge.db (dims=1024)\n",
      "2026-01-15 04:15:43,107 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:43,108 - __main__ - INFO - CHECKING MEMORY SEEDING STATUS\n",
      "2026-01-15 04:15:43,108 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:43,318 - __main__ - WARNING - Force reseed enabled - clearing existing memory\n",
      "2026-01-15 04:15:43,424 - __main__ - INFO - Memory cleared and reinitialized\n",
      "2026-01-15 04:15:43,424 - __main__ - INFO - Loading shots from ./../data/complete_shots.json\n",
      "2026-01-15 04:15:43,427 - __main__ - INFO - Found 20 shots to seed\n",
      "2026-01-15 04:15:43,428 - __main__ - INFO -   Processing: Project Management System\n",
      "2026-01-15 04:15:43,428 - __main__ - INFO -   Processing: Hollywood Approach\n",
      "2026-01-15 04:15:43,428 - __main__ - INFO -   Processing: Word Processor\n",
      "2026-01-15 04:15:43,428 - __main__ - INFO -   Processing: Patient Record and Scheduling\n",
      "2026-01-15 04:15:43,429 - __main__ - INFO -   Processing: Movie-Shop\n",
      "2026-01-15 04:15:43,429 - __main__ - INFO -   Processing: Flights\n",
      "2026-01-15 04:15:43,429 - __main__ - INFO -   Processing: Bank System\n",
      "2026-01-15 04:15:43,429 - __main__ - INFO -   Processing: Veterinary Clinic\n",
      "2026-01-15 04:15:43,429 - __main__ - INFO -   Processing: Auto Repair\n",
      "2026-01-15 04:15:43,429 - __main__ - INFO -   Processing: Restaurant\n",
      "2026-01-15 04:15:43,430 - __main__ - INFO -   Processing: Deliveries\n",
      "2026-01-15 04:15:43,430 - __main__ - INFO -   Processing: Furniture Factory Management\n",
      "2026-01-15 04:15:43,430 - __main__ - INFO -   Processing: Industrial Factory Operations\n",
      "2026-01-15 04:15:43,430 - __main__ - INFO -   Processing: Bycicle Rental\n",
      "2026-01-15 04:15:43,431 - __main__ - INFO -   Processing: Car Park Access System\n",
      "2026-01-15 04:15:43,431 - __main__ - INFO -   Processing: Banking Organizational Structure\n",
      "2026-01-15 04:15:43,431 - __main__ - INFO -   Processing: Prepaid Cell Phone (Decorator Pattern)\n",
      "2026-01-15 04:15:43,431 - __main__ - INFO -   Processing: Library Management System\n",
      "2026-01-15 04:15:43,431 - __main__ - INFO -   Processing: MyDoctor Appointment Management\n",
      "2026-01-15 04:15:43,431 - __main__ - INFO -   Processing: Online Shopping System\n",
      "2026-01-15 04:15:49,355 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:49,358 - __main__ - INFO - ✓ Successfully seeded 20 shots to memory\n",
      "2026-01-15 04:15:49,359 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:49,359 - __main__ - INFO - Long-term memory (SQLite + sqlite-vec) enabled\n",
      "2026-01-15 04:15:49,360 - __main__ - INFO - Seeded 20 few-shot examples into memory\n",
      "2026-01-15 04:15:49,360 - __main__ - INFO - Building LangGraph workflow...\n",
      "2026-01-15 04:15:49,362 - __main__ - INFO - UMLNodes initialized\n",
      "2026-01-15 04:15:49,362 - __main__ - INFO - Creating UML generation workflow\n",
      "2026-01-15 04:15:49,519 - __main__ - INFO - Workflow graph created successfully\n",
      "2026-01-15 04:15:49,566 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:49,566 - __main__ - INFO - SYSTEM INITIALIZED SUCCESSFULLY\n",
      "2026-01-15 04:15:49,566 - __main__ - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System ready for diagram generation\n",
      "Long-term memory: ENABLED\n"
     ]
    }
   ],
   "source": [
    "def initialize_system(\n",
    "    config: Optional[SystemConfig] = None,\n",
    "    enable_long_term_memory: bool = True\n",
    ") -> Tuple[UMLNodes, Any, SystemConfig, Optional[MemoryManager]]:\n",
    "    \"\"\"\n",
    "    Initialize all system components.\n",
    "    \n",
    "    Args:\n",
    "        config: Optional system configuration\n",
    "        enable_long_term_memory: Whether to enable long-term memory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (nodes, compiled_workflow, config, memory_manager)\n",
    "    \"\"\"\n",
    "    cfg = config or SystemConfig()\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"INITIALIZING UML GENERATION SYSTEM\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Creating LLM connection...\")\n",
    "        llm = create_llm(cfg)\n",
    "        \n",
    "        logger.info(\"Initializing PlantUML tool...\")\n",
    "        puml_tool = PlantUMLTool(cfg.plantuml_host)\n",
    "        \n",
    "        memory_mgr = None\n",
    "        if enable_long_term_memory:\n",
    "            logger.info(f\"Initializing long-term memory with {cfg.embedder_model}...\")\n",
    "\n",
    "            dims = 1024 if \"large\" in cfg.embedder_model.lower() else 384\n",
    "            \n",
    "            memory_mgr = MemoryManager(\n",
    "                embedder=SentenceTransformer(cfg.embedder_model),\n",
    "                db_path=cfg.db_path,\n",
    "                embedding_dims=dims\n",
    "            )\n",
    "\n",
    "            seeded_count = seed_memory_from_shots(\n",
    "                memory_manager=memory_mgr,\n",
    "                shots_json_path=cfg.shots_json_path,\n",
    "                force_reseed=True \n",
    "            )\n",
    "\n",
    "            logger.info(\"Long-term memory (SQLite + sqlite-vec) enabled\")\n",
    "\n",
    "            if seeded_count > 0:\n",
    "                logger.info(f\"Seeded {seeded_count} few-shot examples into memory\")\n",
    "        else:\n",
    "            logger.info(\"Long-term memory disabled\")\n",
    "        \n",
    "\n",
    "        logger.info(\"Building LangGraph workflow...\")\n",
    "        nodes = UMLNodes(llm, puml_tool, memory_mgr, cfg)\n",
    "        app = create_uml_graph(nodes, cfg)\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"SYSTEM INITIALIZED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        return nodes, app, cfg, memory_mgr\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"System initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "nodes, app, config, memory_manager = initialize_system(enable_long_term_memory=True)\n",
    "print(\"\\nSystem ready for diagram generation\")\n",
    "print(f\"Long-term memory: {'ENABLED' if memory_manager else 'DISABLED'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f9633429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 04:15:49,695 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:49,695 - __main__ - INFO - RUNNING: Exercise 3\n",
      "2026-01-15 04:15:49,696 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:15:49,696 - __main__ - INFO - Requirements preview: A library system manages books, members, and loans.\n",
      "Books have an ISBN, title, author, publisher, publication year, and availability status.\n",
      "Members h...\n",
      "2026-01-15 04:15:49,714 - __main__ - INFO - --- NODE: RETRIEVE ---\n",
      "2026-01-15 04:15:54,171 - __main__ - INFO - Retrieved 3 similar diagrams from SQLite\n",
      "2026-01-15 04:15:54,177 - __main__ - INFO - Retrieved 3 relevant examples from unified memory\n",
      "2026-01-15 04:15:54,182 - __main__ - INFO - --- NODE: DECOMPOSE ---\n",
      "2026-01-15 04:17:05,051 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:17:05,148 - __main__ - INFO - Decomposition completed\n",
      "2026-01-15 04:17:05,150 - __main__ - INFO - --- NODE: PLAN_AUDIT ---\n",
      "2026-01-15 04:17:19,929 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:17:19,940 - __main__ - INFO - Plan audit: LLM returned is_valid=False with empty critique, overriding to valid\n",
      "2026-01-15 04:17:19,967 - __main__ - INFO - Plan audit completed: valid=True\n",
      "2026-01-15 04:17:19,968 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-15 04:18:33,780 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:18:33,802 - __main__ - INFO - Generation completed (iteration 1)\n",
      "2026-01-15 04:18:33,811 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-15 04:18:33,812 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-15 04:18:34,349 - __main__ - INFO - Syntax validation passed (PNG rendered)\n",
      "2026-01-15 04:18:34,353 - __main__ - INFO - Syntax valid. View at: http://localhost:8080/png/NL5BRWCX3Drp2YumDwYgAAegHMgxoGayWvD10Op0ZeIQtBt0J9fELtx-FJ_RkyoGMBnJauESzPxeeh-LreVZ_giQjkomMX2UALKGPNGsp_YB9s1BGHU45e8hM0UZTPQBhjmiMTrNxdUOn77vH3zYwaCCnZhTO-3ge8-E2g8-l3rRRC1UUMe0F4alZL1Qq_y17mIXLqypJXSIrWQugJU23vYG9ON5U_JzKGJ69Cov8j-3URQn2T8wW6_zSQQg94XBThgppPoM6sn8ZoocQMoiCT4vWTTqgfB5b1hAQxelszSSXjU-WUfhDAyzu-ajAjKgzkKs3DkBRZBhTBN3OEftVm0=\n",
      "2026-01-15 04:18:34,356 - __main__ - INFO - --- NODE: CRITIC ---\n",
      "2026-01-15 04:19:49,906 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-15 04:19:49,945 - __main__ - INFO - Logic validation (NEW): PASSED (Weighted Score: 8.60)\n",
      "2026-01-15 04:19:49,946 - __main__ - INFO -   Requirements Coverage: 8.00/10\n",
      "2026-01-15 04:19:49,946 - __main__ - INFO -   Design Best Practices: 10.00/10\n",
      "2026-01-15 04:19:49,946 - __main__ - INFO -   Structural Integrity: 8.00/10\n",
      "2026-01-15 04:19:49,946 - __main__ - INFO - Storing first valid diagram as best\n",
      "2026-01-15 04:19:49,949 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:19:49,950 - __main__ - INFO - WORKFLOW COMPLETED\n",
      "2026-01-15 04:19:49,950 - __main__ - INFO - ============================================================\n",
      "2026-01-15 04:19:49,950 - __main__ - INFO - Iterations: 1\n",
      "2026-01-15 04:19:49,951 - __main__ - INFO - Syntax Valid: True\n",
      "2026-01-15 04:19:49,951 - __main__ - INFO - Logic Valid: True\n",
      "2026-01-15 04:19:49,954 - __main__ - INFO - PlantUML tool initialized with host: http://localhost:8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Iterations: 1\n",
      "Syntax Valid: True\n",
      "Logic Valid: True\n",
      "\n",
      "Diagram URL: http://localhost:8080/png/NL5BRWCX3Drp2YumDwYgAAegHMgxoGayWvD10Op0ZeIQtBt0J9fELtx-FJ_RkyoGMBnJauESzPxeeh-LreVZ_giQjkomMX2UALKGPNGsp_YB9s1BGHU45e8hM0UZTPQBhjmiMTrNxdUOn77vH3zYwaCCnZhTO-3ge8-E2g8-l3rRRC1UUMe0F4alZL1Qq_y17mIXLqypJXSIrWQugJU23vYG9ON5U_JzKGJ69Cov8j-3URQn2T8wW6_zSQQg94XBThgppPoM6sn8ZoocQMoiCT4vWTTqgfB5b1hAQxelszSSXjU-WUfhDAyzu-ajAjKgzkKs3DkBRZBhTBN3OEftVm0=\n",
      "\n",
      "Generated Diagram:\n",
      "@startuml\n",
      "class Book {\n",
      "  ISBN\n",
      "  title\n",
      "  author\n",
      "  publisher\n",
      "  publication year\n",
      "  availability status\n",
      "}\n",
      "class FacultyMember {\n",
      "  department\n",
      "  employee ID\n",
      "}\n",
      "class Fine {\n",
      "  fine amount\n",
      "  payment status\n",
      "}\n",
      "class Loan {\n",
      "  checkout date\n",
      "  due date\n",
      "  return date\n",
      "}\n",
      "class Member {\n",
      "  address\n",
      "  membership ID\n",
      "  name\n",
      "  phone number\n",
      "  registration date\n",
      "}\n",
      "class Student {\n",
      "  program of study\n",
      "  student ID\n",
      "}\n",
      "Fine --> Loan\n",
      "Loan --> Book\n",
      "Loan --> Member\n",
      "Member <|-- FacultyMember\n",
      "Member <|-- Student\n",
      "@enduml\n"
     ]
    }
   ],
   "source": [
    "def run_single_test(\n",
    "    app: Any,\n",
    "    requirements: str,\n",
    "    exercise_name: str = \"Test Exercise\"\n",
    ") -> AgentState:\n",
    "    \"\"\"\n",
    "    Run the workflow on a single exercise.\n",
    "    \n",
    "    Args:\n",
    "        app: Compiled LangGraph workflow\n",
    "        requirements: Software requirements text\n",
    "        exercise_name: Name for logging purposes\n",
    "        \n",
    "    Returns:\n",
    "        Final workflow state\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"RUNNING: {exercise_name}\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Requirements preview: {requirements[:150]}...\")\n",
    "    \n",
    "    initial_state = create_initial_state(requirements)\n",
    "    \n",
    "    try:\n",
    "        final_output = app.invoke(initial_state, config={\"recursion_limit\": 50})\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"WORKFLOW COMPLETED\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Iterations: {final_output['iterations']}\")\n",
    "        logger.info(f\"Syntax Valid: {final_output['syntax_valid']}\")\n",
    "        logger.info(f\"Logic Valid: {final_output['logic_valid']}\")\n",
    "        \n",
    "        if final_output.get('best_diagram') and not final_output['logic_valid']:\n",
    "            if final_output['best_diagram'] != final_output['current_diagram']:\n",
    "                logger.info(\"Using BEST diagram instead of final (prevented regression)\")\n",
    "                final_output['current_diagram'] = final_output['best_diagram']\n",
    "        \n",
    "        return final_output\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Workflow execution failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Select and run a test exercise\n",
    "test_idx = 2\n",
    "requirements = test_exercises[test_idx][\"requirements\"]\n",
    "\n",
    "final_output = run_single_test(\n",
    "    app, \n",
    "    requirements, \n",
    "    f\"Exercise {test_idx + 1}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Iterations: {final_output['iterations']}\")\n",
    "print(f\"Syntax Valid: {final_output['syntax_valid']}\")\n",
    "print(f\"Logic Valid: {final_output['logic_valid']}\")\n",
    "\n",
    "if final_output['current_diagram']:\n",
    "    puml_tool = PlantUMLTool(config.plantuml_host)\n",
    "    diagram_url = puml_tool.get_diagram_url(final_output['current_diagram'])\n",
    "    print(f\"\\nDiagram URL: {diagram_url}\")\n",
    "    \n",
    "    print(\"\\nGenerated Diagram:\")\n",
    "    print(final_output['current_diagram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1e84c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@startuml\n",
      "class Book {\n",
      "  ISBN\n",
      "  title\n",
      "  author\n",
      "  publisher\n",
      "  publication year\n",
      "  availability status\n",
      "}\n",
      "class FacultyMember {\n",
      "  department\n",
      "  employee ID\n",
      "}\n",
      "class Fine {\n",
      "  fine amount\n",
      "  payment status\n",
      "}\n",
      "class Loan {\n",
      "  checkout date\n",
      "  due date\n",
      "  return date\n",
      "}\n",
      "class Member {\n",
      "  address\n",
      "  membership ID\n",
      "  name\n",
      "  phone number\n",
      "  registration date\n",
      "}\n",
      "class Student {\n",
      "  program of study\n",
      "  student ID\n",
      "}\n",
      "Fine --> Loan\n",
      "Loan --> Book\n",
      "Loan --> Member\n",
      "Member <|-- FacultyMember\n",
      "Member <|-- Student\n",
      "@enduml\n"
     ]
    }
   ],
   "source": [
    "print(final_output['current_diagram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8b3e9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics(BaseModel):\n",
    "    \"\"\"Container for evaluation metrics.\"\"\"\n",
    "    precision: float = Field(ge=0.0, le=1.0, description=\"Precision score\")\n",
    "    recall: float = Field(ge=0.0, le=1.0, description=\"Recall score\")\n",
    "    f1: float = Field(ge=0.0, le=1.0, description=\"F1 score\")\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"P={self.precision:.2f}, R={self.recall:.2f}, F1={self.f1:.2f}\"\n",
    "\n",
    "\n",
    "class PlantUMLParser:\n",
    "    \"\"\"\n",
    "    Parser for extracting structured information from PlantUML diagrams.\n",
    "    \n",
    "    Extracts classes, attributes, and relationships from PlantUML code\n",
    "    for evaluation purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, plantuml_code: str):\n",
    "        \"\"\"\n",
    "        Initialize parser with PlantUML code.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: PlantUML diagram code\n",
    "        \"\"\"\n",
    "        self.plantuml_code = plantuml_code\n",
    "        self.classes: Dict[str, Dict[str, List[str]]] = {}\n",
    "        self.relationships: List[Dict[str, Any]] = []\n",
    "        self.parse()\n",
    "    \n",
    "    def parse(self) -> None:\n",
    "        \"\"\"Parse the PlantUML code.\"\"\"\n",
    "        try:\n",
    "            self._extract_classes()\n",
    "            self._extract_relationships()\n",
    "            logger.debug(f\"Parsed {len(self.classes)} classes and {len(self.relationships)} relationships\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing failed: {e}\")\n",
    "    \n",
    "    def _extract_classes(self) -> None:\n",
    "        \"\"\"Extract class definitions and their attributes.\"\"\"\n",
    "        class_pattern = r'class\\s+(\\w+)\\s*\\{([^}]*)\\}'\n",
    "        matches = re.finditer(class_pattern, self.plantuml_code, re.MULTILINE | re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            class_name = match.group(1)\n",
    "            class_body = match.group(2)\n",
    "            \n",
    "            attributes = []\n",
    "            for line in class_body.strip().split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('--'):\n",
    "                    attributes.append(line)\n",
    "            \n",
    "            self.classes[class_name] = {'attributes': attributes}\n",
    "    \n",
    "    def _extract_relationships(self) -> None:\n",
    "        \"\"\"Extract relationships between classes with cardinalities.\"\"\"\n",
    "        patterns = [\n",
    "            # Generalization (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*<\\|--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'generalization'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--|>\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'generalization'),\n",
    "            \n",
    "            # Composition (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*\\*--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'composition'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--\\*\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'composition'),\n",
    "            \n",
    "            # Aggregation (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*o--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'aggregation'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--o\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'aggregation'),\n",
    "            \n",
    "            # Directed Association (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*-->\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'association'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*<--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'association'),\n",
    "            \n",
    "            # Simple Association (no arrow)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'association'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            for match in re.finditer(pattern, self.plantuml_code):\n",
    "                source = match.group(1)\n",
    "                target = match.group(4)\n",
    "                \n",
    "                # Skip if source or target is None or empty\n",
    "                if not source or not target:\n",
    "                    continue\n",
    "                \n",
    "                self.relationships.append({\n",
    "                    'type': rel_type,\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'cardinality_source': match.group(2) if match.lastindex >= 2 else None,\n",
    "                    'cardinality_target': match.group(3) if match.lastindex >= 3 else None\n",
    "                })\n",
    "\n",
    "\n",
    "class DiagramEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for comparing generated diagrams against gold standards.\n",
    "    \n",
    "    Computes precision, recall, and F1 scores for classes, attributes,\n",
    "    and relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gold_plantuml: str, pred_plantuml: str):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with gold and predicted diagrams.\n",
    "        \n",
    "        Args:\n",
    "            gold_plantuml: Gold standard PlantUML code\n",
    "            pred_plantuml: Predicted PlantUML code\n",
    "        \"\"\"\n",
    "        self.gold_parser = PlantUMLParser(gold_plantuml)\n",
    "        self.pred_parser = PlantUMLParser(pred_plantuml)\n",
    "    \n",
    "    def _normalize_attr(self, attr_str: str) -> str:\n",
    "        \"\"\"Normalize attribute strings for comparison.\"\"\"\n",
    "        return attr_str.split(':')[0].strip().lower()\n",
    "    \n",
    "    def _normalize_rel_type(self, rel_type: str) -> str:\n",
    "        \"\"\"Normalize relationship types.\"\"\"\n",
    "        mapping = {\n",
    "            '<|--': 'INHERITANCE',\n",
    "            '--|>': 'INHERITANCE',\n",
    "            '*--': 'COMPOSITION',\n",
    "            '--*': 'COMPOSITION',\n",
    "            'o--': 'AGGREGATION',\n",
    "            '--o': 'AGGREGATION',\n",
    "            '--': 'ASSOCIATION',\n",
    "            '<--': 'ASSOCIATION',\n",
    "            '-->': 'ASSOCIATION'\n",
    "        }\n",
    "        return mapping.get(rel_type, 'ASSOCIATION')\n",
    "    \n",
    "    def _calculate_metrics(\n",
    "        self, \n",
    "        gold_set: set, \n",
    "        pred_set: set\n",
    "    ) -> EvaluationMetrics:\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1 scores.\n",
    "        \n",
    "        Args:\n",
    "            gold_set: Set of gold standard elements\n",
    "            pred_set: Set of predicted elements\n",
    "            \n",
    "        Returns:\n",
    "            EvaluationMetrics object\n",
    "        \"\"\"\n",
    "        tp = len(gold_set.intersection(pred_set))\n",
    "        fp = len(pred_set - gold_set)\n",
    "        fn = len(gold_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return EvaluationMetrics(\n",
    "            precision=round(precision, 2),\n",
    "            recall=round(recall, 2),\n",
    "            f1=round(f1, 2)\n",
    "        )\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, EvaluationMetrics]:\n",
    "        \"\"\"\n",
    "        Get all evaluation metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with metrics for classes, attributes, and relationships\n",
    "        \"\"\"\n",
    "        # Classes \n",
    "        gold_classes = {c.lower() for c in self.gold_parser.classes.keys()}\n",
    "        pred_classes = {c.lower() for c in self.pred_parser.classes.keys()}\n",
    "        \n",
    "        # Attributes \n",
    "        gold_attrs = set()\n",
    "        for cls, info in self.gold_parser.classes.items():\n",
    "            for attr in info['attributes']:\n",
    "                gold_attrs.add((cls.lower(), self._normalize_attr(attr)))\n",
    "        \n",
    "        pred_attrs = set()\n",
    "        for cls, info in self.pred_parser.classes.items():\n",
    "            for attr in info['attributes']:\n",
    "                pred_attrs.add((cls.lower(), self._normalize_attr(attr)))\n",
    "        \n",
    "        # Relationships (type + direction only)\n",
    "        gold_rels = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']))\n",
    "            for r in self.gold_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        pred_rels = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']))\n",
    "            for r in self.pred_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        \n",
    "        # Cardinalities \n",
    "        gold_rels_card = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']),\n",
    "             (r['cardinality_source'] or '').strip(), (r['cardinality_target'] or '').strip())\n",
    "            for r in self.gold_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        pred_rels_card = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']),\n",
    "             (r['cardinality_source'] or '').strip(), (r['cardinality_target'] or '').strip())\n",
    "            for r in self.pred_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"classes\": self._calculate_metrics(gold_classes, pred_classes),\n",
    "            \"attributes\": self._calculate_metrics(gold_attrs, pred_attrs),\n",
    "            \"relationships\": self._calculate_metrics(gold_rels, pred_rels),\n",
    "            \"cardinalities\": self._calculate_metrics(gold_rels_card, pred_rels_card)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def evaluate_diagram(\n",
    "    gold_standard: str,\n",
    "    generated_diagram: str\n",
    ") -> Dict[str, EvaluationMetrics]:\n",
    "    \"\"\"\n",
    "    Evaluate a generated diagram against gold standard.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: Gold standard PlantUML code\n",
    "        generated_diagram: Generated PlantUML code\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    evaluator = DiagramEvaluator(gold_standard, generated_diagram)\n",
    "    return evaluator.get_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "389c5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION METRICS\n",
      "============================================================\n",
      "\n",
      "Classes:       P=0.83, R=0.71, F1=0.77\n",
      "Attributes:    P=0.30, R=0.27, F1=0.29\n",
      "Relationships: P=0.20, R=0.14, F1=0.17\n",
      "Cardinalities: P=0.20, R=0.14, F1=0.17\n",
      "\n",
      "============================================================\n",
      "OVERALL F1 SCORE: 0.37\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "gold_standard = test_exercises[test_idx][\"solution_plantuml\"]\n",
    "generated_diagram = final_output[\"current_diagram\"]\n",
    "\n",
    "metrics = evaluate_diagram(gold_standard, generated_diagram)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nClasses:       {metrics['classes']}\")\n",
    "print(f\"Attributes:    {metrics['attributes']}\")\n",
    "print(f\"Relationships: {metrics['relationships']}\")\n",
    "print(f\"Cardinalities: {metrics['cardinalities']}\")\n",
    "\n",
    "weighted_avg_f1 = (\n",
    "    metrics['classes'].f1 * 0.3 + \n",
    "    metrics['attributes'].f1 * 0.2 + \n",
    "    metrics['relationships'].f1 * 0.3 +\n",
    "    metrics['cardinalities'].f1 * 0.2\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL F1 SCORE: {weighted_avg_f1:.2f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "42196e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchResult(BaseModel):\n",
    "    \"\"\"Result from a single exercise in batch evaluation.\"\"\"\n",
    "    exercise_num: int = Field(description=\"Exercise number\")\n",
    "    success: bool = Field(description=\"Whether the exercise was successful\")\n",
    "    iterations: int = Field(default=0, ge=0, description=\"Number of iterations used\")\n",
    "    syntax_valid: bool = Field(default=False, description=\"Whether syntax validation passed\")\n",
    "    logic_valid: bool = Field(default=False, description=\"Whether logic validation passed\")\n",
    "    metrics: Optional[Dict[str, EvaluationMetrics]] = Field(default=None, description=\"Evaluation metrics\")\n",
    "    diagram_url: Optional[str] = Field(default=None, description=\"URL to view the diagram\")\n",
    "    error: Optional[str] = Field(default=None, description=\"Error message if failed\")\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for DataFrame creation.\"\"\"\n",
    "        if self.metrics:\n",
    "            return {\n",
    "                \"exercise\": self.exercise_num,\n",
    "                \"success\": self.success,\n",
    "                \"iterations\": self.iterations,\n",
    "                \"syntax_valid\": self.syntax_valid,\n",
    "                \"logic_valid\": self.logic_valid,\n",
    "                \"class_f1\": self.metrics['classes'].f1,\n",
    "                \"attr_f1\": self.metrics['attributes'].f1,\n",
    "                \"rel_f1\": self.metrics['relationships'].f1,\n",
    "                \"card_f1\": self.metrics['cardinalities'].f1,\n",
    "                \"diagram_url\": self.diagram_url\n",
    "            }\n",
    "        return {\n",
    "            \"exercise\": self.exercise_num,\n",
    "            \"success\": self.success,\n",
    "            \"error\": self.error\n",
    "        }\n",
    "\n",
    "\n",
    "def evaluate_batch(\n",
    "    app: Any,\n",
    "    test_exercises: List[Dict[str, Any]],\n",
    "    puml_tool: PlantUMLTool,\n",
    "    max_exercises: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run batch evaluation on multiple exercises.\n",
    "    \n",
    "    Args:\n",
    "        app: Compiled LangGraph workflow\n",
    "        test_exercises: List of exercise dictionaries\n",
    "        puml_tool: PlantUML tool for URL generation\n",
    "        max_exercises: Optional limit on number of exercises\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    exercises_to_test = test_exercises[:max_exercises] if max_exercises else test_exercises\n",
    "    results = []\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"BATCH EVALUATION: {len(exercises_to_test)} exercises\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    for i, exercise in enumerate(exercises_to_test):\n",
    "        logger.info(f\"\\n--- Exercise {i+1}/{len(exercises_to_test)} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Run workflow\n",
    "            requirements = exercise[\"requirements\"]\n",
    "            final_output = run_single_test(app, requirements, f\"Exercise {i+1}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            gold_standard = exercise[\"solution_plantuml\"]\n",
    "            generated_diagram = final_output[\"current_diagram\"]\n",
    "            metrics = evaluate_diagram(gold_standard, generated_diagram)\n",
    "            \n",
    "            result = BatchResult(\n",
    "                exercise_num=i + 1,\n",
    "                success=True,\n",
    "                iterations=final_output[\"iterations\"],\n",
    "                syntax_valid=final_output[\"syntax_valid\"],\n",
    "                logic_valid=final_output[\"logic_valid\"],\n",
    "                metrics=metrics,\n",
    "                diagram_url=puml_tool.get_diagram_url(generated_diagram)\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Exercise {i+1}: F1 = Classes:{metrics['classes'].f1:.2f} | \"\n",
    "                       f\"Attrs:{metrics['attributes'].f1:.2f} | Rels:{metrics['relationships'].f1:.2f} | \"\n",
    "                       f\"Cards:{metrics['cardinalities'].f1:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Exercise {i+1} failed: {e}\")\n",
    "            result = BatchResult(\n",
    "                exercise_num=i + 1,\n",
    "                success=False,\n",
    "                error=str(e)\n",
    "            )\n",
    "        \n",
    "        results.append(result.to_dict())\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"BATCH EVALUATION COMPLETE\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Example: Run on first 3 exercises (uncomment to execute)\n",
    "# df_results = evaluate_batch(app, test_exercises, puml_tool, max_exercises=3)\n",
    "# \n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"BATCH EVALUATION SUMMARY\")\n",
    "# print(\"=\"*60)\n",
    "# successful = df_results[df_results['success'] == True]\n",
    "# if not successful.empty:\n",
    "    # print(successful[['exercise', 'class_f1', 'attr_f1', 'rel_f1']].to_string(index=False))\n",
    "    # avg_f1 = successful[['class_f1', 'attr_f1', 'rel_f1']].mean().mean()\n",
    "    # print(f\"\\nAverage F1: {avg_f1:.2f}\")\n",
    "# else:\n",
    "    # print(\"No successful evaluations\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
