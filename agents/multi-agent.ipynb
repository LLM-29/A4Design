{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "e903b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import zlib\n",
    "import base64\n",
    "import requests\n",
    "import operator\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# sqlite-vec expects the standard library sqlite3 module.\n",
    "# Older versions of this notebook replaced sqlite3 with sqlean via sys.modules; undo that if present.\n",
    "if sys.modules.get(\"sqlite3\") is sys.modules.get(\"sqlean\"):\n",
    "    del sys.modules[\"sqlite3\"]\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Annotated, List, TypedDict, Optional, Dict, Any, Tuple, Literal\n",
    "from pydantic import BaseModel, Field, computed_field, field_validator, model_validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import SQLiteVec\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from prompts import DECOMPOSER_SYSTEM, GENERATOR_SYSTEM, CRITIC_SYSTEM, REFLECTOR_SYSTEM, PLAN_AUDITOR_SYSTEM, STRUCTURE_REFINER_SYSTEM\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "6e7d7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeNames(str, Enum):\n",
    "    \"\"\"Enum for node names to avoid string literals.\"\"\"\n",
    "    RETRIEVE = \"retrieve\"\n",
    "    DECOMPOSE = \"decompose\"\n",
    "    GENERATE = \"generate\"\n",
    "    SYNTAX_CHECK = \"syntax_check\"\n",
    "    CRITIC = \"critic\"\n",
    "    REFLECTOR = \"reflector\"\n",
    "    PLAN_AUDIT = \"plan_audit\"\n",
    "    STRUCTURE_REFINER = \"structure_refiner\"\n",
    "\n",
    "\n",
    "class Attribute(BaseModel):\n",
    "    \"\"\"Model for a class attribute.\"\"\"\n",
    "    name: str = Field(description=\"Attribute name\")\n",
    "    type: str = Field(description=\"Attribute type\")\n",
    "\n",
    "\n",
    "class Class(BaseModel):\n",
    "    \"\"\"Model for a UML class.\"\"\"\n",
    "    name: str = Field(description=\"Class name\")\n",
    "    attributes: List[Attribute] = Field(default_factory=list, description=\"List of class attributes\")\n",
    "\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Model for a relationship between classes.\"\"\"\n",
    "    source: str = Field(description=\"Source class name\")\n",
    "    target: str = Field(description=\"Target class name\")\n",
    "    type: str = Field(description=\"Relationship type (e.g., association, composition, inheritance)\")\n",
    "    source_multiplicity: str = Field(description=\"Multiplicity at source end\")\n",
    "    target_multiplicity: str = Field(description=\"Multiplicity at target end\")\n",
    "\n",
    "\n",
    "class DecompositionResult(BaseModel):\n",
    "    \"\"\"Structured output from the DECOMPOSE node.\"\"\"\n",
    "    classes: List[Class] = Field(default_factory=list, description=\"List of identified classes\")\n",
    "    relationships: List[Relationship] = Field(default_factory=list, description=\"List of relationships between classes\")\n",
    "\n",
    "\n",
    "class PlanAudit(BaseModel):\n",
    "    critique: List[Literal[\n",
    "        \"Missing class\",\n",
    "        \"Missing relationship\",\n",
    "        \"Disconnected class\"\n",
    "    ]] = Field(default_factory=list, description=\"List of critique points found in the plan.\")\n",
    "    suggestions: List[str] = Field(default_factory=list, description=\"Actionable steps to fix the plan.\")\n",
    "    \n",
    "    @computed_field\n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        return len(self.critique) == 0\n",
    "\n",
    "\n",
    "class SystemConfig(BaseModel):\n",
    "    \"\"\"System configuration for UML generation.\"\"\"\n",
    "    lmstudio_base_url: str = Field(default=\"http://localhost:1234/v1\", description=\"LMStudio API endpoint\")\n",
    "    model_name: str = Field(default=\"mistralai/devstral-small-2-2512\", description=\"Model to use\")\n",
    "    embedder_model: str = Field(default=\"BAAI/bge-large-en-v1.5\", description=\"Embedder model for semantic search\")\n",
    "    db_path: str = Field(default=\"./../data/uml_knowledge.db\", description=\"Path to SQLite database\")\n",
    "    shots_json_path: str = Field(default=\"./../data/complete_shots.json\", description=\"Path to few-shot examples\")\n",
    "    plantuml_host: str = Field(default=\"http://localhost:8080\", description=\"PlantUML server host\")\n",
    "    max_iterations: int = Field(default=6, ge=1, description=\"Maximum workflow iterations\")\n",
    "    max_tokens_decompose: int = Field(default=2048, description=\"Max tokens for decompose step\")\n",
    "    max_tokens_generate: int = Field(default=2048, description=\"Max tokens for generate step\")\n",
    "    max_tokens_critique: int = Field(default=2048, description=\"Max tokens for critique step\")\n",
    "    max_tokens_reflect: int = Field(default=2048, description=\"Max tokens for reflect step\")\n",
    "    max_tokens_refine: int = Field(default=2048, description=\"Max tokens for structure refine step\")\n",
    "    temperature: float = Field(default=0.15, ge=0.0, le=2.0, description=\"Base temperature for LLM\")\n",
    "    num_few_shots: int = Field(default=3, ge=0, description=\"Number of few-shot examples\")\n",
    "    request_timeout: int = Field(default=5, ge=1, description=\"Timeout for PlantUML server requests\")\n",
    "    llm_timeout: int = Field(default=600, ge=1, description=\"Timeout for LLM operations\")\n",
    "    # New fields for iteration metrics\n",
    "    plateau_window: int = Field(default=3, ge=2, description=\"Number of iterations to consider for plateau detection\")\n",
    "    plateau_threshold: float = Field(default=0.1, ge=0.0, description=\"Score delta threshold for plateau detection\")\n",
    "    max_stagnant_iterations: int = Field(default=2, ge=1, description=\"Max consecutive stagnant iterations before stopping\")\n",
    "\n",
    "\n",
    "class PlantUMLResult(BaseModel):\n",
    "    \"\"\"Result from PlantUML validation.\"\"\"\n",
    "    is_valid: bool = Field(description=\"Whether the PlantUML syntax is valid\")\n",
    "    error: Optional[str] = Field(default=None, description=\"Error message if validation failed\")\n",
    "    url: Optional[str] = Field(default=None, description=\"URL to view the diagram\")\n",
    "    svg_url: Optional[str] = Field(default=None, description=\"URL to view the diagram as SVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "8b32b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm(config: Optional[SystemConfig] = None) -> ChatOpenAI:\n",
    "    \"\"\"\n",
    "    Create a ChatOpenAI instance configured for LMStudio.\n",
    "    \n",
    "    Args:\n",
    "        config: Optional system configuration\n",
    "        \n",
    "    Returns:\n",
    "        Configured ChatOpenAI instance\n",
    "    \"\"\"\n",
    "    cfg = config or SystemConfig()\n",
    "    logger.info(f\"Connecting to LMStudio at {cfg.lmstudio_base_url}\")\n",
    "    logger.info(f\"Using model: {cfg.model_name} (temp={cfg.temperature})\")\n",
    "    \n",
    "    return ChatOpenAI(\n",
    "        base_url=cfg.lmstudio_base_url,\n",
    "        api_key=\"lm-studio\",  \n",
    "        model=cfg.model_name,\n",
    "        temperature=cfg.temperature,\n",
    "        timeout=cfg.llm_timeout \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "205fd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixability(str, Enum):\n",
    "    render_only = \"render_only\"\n",
    "    structure_change = \"structure_change\"\n",
    "    unfixable = \"unfixable\"\n",
    "\n",
    "\n",
    "class Severity(str, Enum):\n",
    "    error = \"error\"\n",
    "    warning = \"warning\"\n",
    "\n",
    "\n",
    "class FindingCategory(str, Enum):\n",
    "    coverage = \"coverage\"         \n",
    "    structure = \"structure\"        \n",
    "    render = \"render\"              \n",
    "    syntax = \"syntax\"            \n",
    "\n",
    "\n",
    "class CritiqueFinding(BaseModel):\n",
    "    id: str = Field(default=None, description=\"Stable identifier for this finding\")\n",
    "    category: FindingCategory\n",
    "    severity: Severity = Field(default=Severity.error, description=\"Severity level of the finding\")\n",
    "    fixability: Fixability\n",
    "    affected_elements: List[str]\n",
    "    description: str\n",
    "    expected_correction: Optional[str] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _slugify(text: str) -> str:\n",
    "        text = re.sub(r\"[^a-z0-9]+\", \"_\", (text or \"\").lower())\n",
    "        return re.sub(r\"_+\", \"_\", text).strip(\"_\") or \"issue\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _entities_from_affected(affected: List[str]) -> Tuple[str, str]:\n",
    "        entity, related = \"none\", \"none\"\n",
    "        for a in affected or []:\n",
    "            if a.startswith(\"class:\"):\n",
    "                entity = a.split(\":\", 1)[1].strip() or entity\n",
    "            elif a.startswith(\"attr:\"):\n",
    "                entity = (a.split(\":\", 1)[1].split(\".\", 1)[0].strip() or entity)\n",
    "            elif a.startswith(\"rel:\"):\n",
    "                rel = a.split(\":\", 1)[1]\n",
    "                parts = re.split(r\"<\\|--|--\\|>|<--|-->|--|\\*--|--\\*\", rel)\n",
    "                if len(parts) >= 2:\n",
    "                    entity = parts[0].strip() or entity\n",
    "                    related = parts[1].strip() or related\n",
    "        return entity or \"none\", related or \"none\"\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def ensure_canonical_id(self):\n",
    "        if self.id and self.id.count(\"::\") == 3 and all(p.strip() for p in self.id.split(\"::\")):\n",
    "            return self\n",
    "\n",
    "        entity, related = self._entities_from_affected(self.affected_elements)\n",
    "\n",
    "        raw = (self.id or \"\").strip()\n",
    "        tail = raw.split(\":\", 1)[1].strip() if \":\" in raw else raw\n",
    "\n",
    "        tail_slug = self._slugify(tail)\n",
    "        if \"missing_attribute\" in tail_slug:\n",
    "            issue_slug = \"missing_attribute\"\n",
    "        elif \"missing_class\" in tail_slug:\n",
    "            issue_slug = \"missing_class\"\n",
    "        elif \"duplicate_relationship\" in tail_slug:\n",
    "            issue_slug = \"duplicate_relationship\"\n",
    "        else:\n",
    "            seed = f\"{self.category.value}|{self.fixability.value}|{','.join(sorted(self.affected_elements or []))}|{self.description[:80]}\"\n",
    "            issue_slug = f\"auto_{hashlib.md5(seed.encode()).hexdigest()[:8]}\"\n",
    "\n",
    "        self.id = f\"{self.category.value}::{entity}::{related}::{issue_slug}\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class CritiqueSummary(BaseModel):\n",
    "    total_findings: int = 0\n",
    "    render_only: int = 0\n",
    "    structure_change: int = 0\n",
    "    unfixable: int = 0\n",
    "    new_findings: int = 0\n",
    "    resolved_findings: int = 0\n",
    "\n",
    "\n",
    "class CritiqueReport(BaseModel):\n",
    "    findings: List[CritiqueFinding]\n",
    "\n",
    "    @computed_field\n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        return not any(f.severity == Severity.error for f in self.findings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "8bd84004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Shared state for the LangGraph workflow.\n",
    "    \"\"\"\n",
    "    requirements: str\n",
    "    plan: Optional[str]\n",
    "    examples: List[Dict[str, str]]\n",
    "    current_diagram: Optional[str]\n",
    "    best_diagram: Optional[str]  \n",
    "    summary: Optional[str]\n",
    "    syntax_valid: bool\n",
    "    logic_valid: bool\n",
    "    error_message: Optional[str]\n",
    "    plan_valid: bool \n",
    "    audit_feedback: Optional[List[str]]  # Feedback from auditor\n",
    "    plan_audit_attempts: int  # audit loop iterations\n",
    "    best_score: float\n",
    "    best_code: str\n",
    "    current_validation: Optional[CritiqueReport]\n",
    "    failed_attempts: Annotated[List[Dict[str, Any]], operator.add]\n",
    "    iterations: int\n",
    "    stagnant_count: int  # Track consecutive iterations without changes\n",
    "    critique_cache: Dict[str, Dict[str, Any]]  # Cache critiques by diagram hash\n",
    "    score_history: List[float]  # History of weighted scores for plateau detection\n",
    "    delta_score: float  # Score change from previous iteration\n",
    "    audit_suggestions: Optional[List[str]]  # Suggestions from plan auditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "e70e9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantUMLTool:\n",
    "    \"\"\"\n",
    "    Tool for validating and rendering PlantUML diagrams.\n",
    "    \n",
    "    This class interfaces with a PlantUML server to check syntax\n",
    "    and generate diagram URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"http://localhost:8080\"):\n",
    "        \"\"\"\n",
    "        Initialize PlantUML tool.\n",
    "        \n",
    "        Args:\n",
    "            host: PlantUML server host URL\n",
    "        \"\"\"\n",
    "        self.host = host\n",
    "        logger.info(f\"PlantUML tool initialized with host: {host}\")\n",
    "\n",
    "    def extract_plantuml(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract PlantUML code from markdown blocks or raw text.\n",
    "        \n",
    "        Args:\n",
    "            text: Text containing PlantUML code\n",
    "            \n",
    "        Returns:\n",
    "            Extracted PlantUML code or empty string\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Try to extract from ```plantuml ... ```\n",
    "        fence_match = re.search(r\"```\\s*plantuml\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if fence_match:\n",
    "            return fence_match.group(1).strip()\n",
    "        \n",
    "        # Try to extract from @startuml ... @enduml\n",
    "        tag_match = re.search(r\"@startuml.*?@enduml\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if tag_match:\n",
    "            return tag_match.group(0).strip()\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def _encode_plantuml(self, plantuml_code: str) -> str:\n",
    "        \"\"\"\n",
    "        Encode PlantUML code for URL.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: Raw PlantUML code\n",
    "            \n",
    "        Returns:\n",
    "            URL-safe encoded string\n",
    "        \"\"\"\n",
    "        code = plantuml_code.strip()\n",
    "        \n",
    "        if not code.startswith(\"@startuml\"): \n",
    "            code = f\"@startuml\\n{code}\"\n",
    "        if not code.endswith(\"@enduml\"): \n",
    "            code = f\"{code}\\n@enduml\"\n",
    "        \n",
    "        compressed = zlib.compress(code.encode('utf-8'))[2:-4]\n",
    "        encoded = base64.b64encode(compressed).translate(\n",
    "            bytes.maketrans(\n",
    "                b\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\",\n",
    "                b\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_\"\n",
    "            )\n",
    "        ).decode('utf-8')\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "    def get_diagram_url(self, plantuml_code: str, format: str = \"png\") -> str:\n",
    "        \"\"\"\n",
    "        Generate a viewable URL for the PlantUML diagram.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: PlantUML diagram code\n",
    "            format: Output format (png, svg, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            URL to view the diagram\n",
    "        \"\"\"\n",
    "        diagram_code = self.extract_plantuml(plantuml_code)\n",
    "        encoded = self._encode_plantuml(diagram_code)\n",
    "        return f\"{self.host}/{format}/{encoded}\"\n",
    "        \n",
    "    def check_syntax(self, plantuml_code: str, timeout: int = 5) -> PlantUMLResult:\n",
    "        \"\"\"\n",
    "        Validate PlantUML syntax with detailed error extraction.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: PlantUML code to validate\n",
    "            timeout: Request timeout in seconds\n",
    "            \n",
    "        Returns:\n",
    "            PlantUMLResult with validation status and detailed error if applicable.\n",
    "        \"\"\"\n",
    "        logger.info(\"Validating PlantUML syntax\")\n",
    "        \n",
    "        try:\n",
    "            diagram_code = self.extract_plantuml(plantuml_code)\n",
    "            encoded = self._encode_plantuml(diagram_code)\n",
    "            \n",
    "            url_png = f\"{self.host}/png/{encoded}\"\n",
    "            response = requests.get(url_png, timeout=timeout)\n",
    "            \n",
    "            if response.status_code == 200 and response.content[:4] == b'\\x89PNG':\n",
    "                logger.info(\"Syntax validation passed (PNG rendered)\")\n",
    "                return PlantUMLResult(\n",
    "                    is_valid=True,\n",
    "                    url=url_png,\n",
    "                    svg_url=f\"{self.host}/svg/{encoded}\"\n",
    "                )\n",
    "            \n",
    "            logger.warning(\"PNG rendering failed. Fetching detailed syntax error...\")\n",
    "            url_txt = f\"{self.host}/txt/{encoded}\"\n",
    "            error_response = requests.get(url_txt, timeout=timeout)\n",
    "            \n",
    "            detailed_error = error_response.text.strip() if error_response.status_code == 200 else \"Unknown server error\"\n",
    "            \n",
    "            error_msg = f\"PlantUML Syntax Error:\\n{detailed_error[:1000]}\"\n",
    "            logger.error(f\"Syntax error detected: {error_msg}\")\n",
    "            \n",
    "            return PlantUMLResult(\n",
    "                is_valid=False,\n",
    "                error=error_msg\n",
    "            )\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            error_msg = f\"PlantUML Server Connection Error: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return PlantUMLResult(is_valid=False, error=error_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error during syntax check: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return PlantUMLResult(is_valid=False, error=error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "2d633d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryManager:\n",
    "    \"\"\"\n",
    "    Manages long-term memory for UML diagram generation using LangChain's SQLiteVec.\n",
    "    \n",
    "    Supports semantic search to find similar past solutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder: SentenceTransformer,\n",
    "        db_path: str = \"./../data/uml_knowledge.db\",\n",
    "        embedding_dims: int = 1024\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize memory manager with LangChain SQLiteVec.\n",
    "        \n",
    "        Args:\n",
    "            embedder: SentenceTransformer model for semantic search\n",
    "            db_path: Path to the SQLite database file\n",
    "            embedding_dims: Dimensions of the embeddings \n",
    "        \"\"\"\n",
    "        self.embedder = embedder\n",
    "        self.db_path = db_path\n",
    "        self.embedding_dims = embedding_dims\n",
    "        \n",
    "\n",
    "        self.embedding_function = HuggingFaceEmbeddings(\n",
    "            model_name=embedder.model_name if hasattr(embedder, 'model_name') else \"BAAI/bge-large-en-v1.5\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Create directory and connection\n",
    "        os.makedirs(os.path.dirname(self.db_path) if os.path.dirname(self.db_path) else \".\", exist_ok=True)\n",
    "        \n",
    "        # Create connection using sqlean instead of default sqlite3 (which lacks extension support on macOS)\n",
    "        try:\n",
    "            import sqlean\n",
    "            import sqlite_vec\n",
    "            \n",
    "            self.connection = sqlean.connect(self.db_path)\n",
    "            self.connection.row_factory = sqlean.Row\n",
    "            self.connection.enable_load_extension(True)\n",
    "            sqlite_vec.load(self.connection)\n",
    "            self.connection.enable_load_extension(False)\n",
    "            logger.info(\"Used sqlean for SQLite connection (extension support enabled)\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"sqlean not found, falling back to SQLiteVec.create_connection (may fail on macOS)\")\n",
    "            self.connection = SQLiteVec.create_connection(db_file=self.db_path)\n",
    "        \n",
    "        # Initialize vector store with connection\n",
    "        self.vector_store = SQLiteVec(\n",
    "            table=\"uml_memories\",\n",
    "            connection=self.connection,\n",
    "            embedding=self.embedding_function\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"MemoryManager initialized with LangChain SQLiteVec at {db_path} (dims={embedding_dims})\")\n",
    "\n",
    "    def save_diagram(\n",
    "        self,\n",
    "        requirements: str,\n",
    "        diagram: str,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Save a validated diagram to SQLite long-term memory.\n",
    "                \n",
    "        Args:\n",
    "            requirements: Original requirements text\n",
    "            diagram: PlantUML diagram code\n",
    "            metadata: Optional metadata\n",
    "            \n",
    "        Returns:\n",
    "            ID of the stored record\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        full_metadata = metadata or {}\n",
    "        full_metadata.update({\n",
    "            \"diagram\": diagram,\n",
    "            \"timestamp\": timestamp\n",
    "        })\n",
    "        \n",
    "\n",
    "        doc = Document(\n",
    "            page_content=requirements,\n",
    "            metadata=full_metadata\n",
    "        )\n",
    "        \n",
    "        ids = self.vector_store.add_documents([doc])\n",
    "        \n",
    "        logger.info(\"Diagram saved to SQLite memory using LangChain SQLiteVec\")\n",
    "        return ids[0] if ids else 0\n",
    "    \n",
    "    def retrieve_similar_diagrams(\n",
    "        self,\n",
    "        requirements: str,\n",
    "        limit: int = 2\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve similar diagrams from SQLite memory using vector search.\n",
    "        \n",
    "        Args:\n",
    "            requirements: Requirements text to search for\n",
    "            limit: Maximum number of results\n",
    "            \n",
    "        Returns:\n",
    "            List of similar diagram records\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.vector_store.similarity_search(requirements, k=limit)\n",
    "            \n",
    "            diagrams = []\n",
    "            for doc in results:\n",
    "                diagrams.append({\n",
    "                    \"requirements\": doc.page_content,\n",
    "                    \"diagram\": doc.metadata.get(\"diagram\", \"\"),\n",
    "                    \"timestamp\": doc.metadata.get(\"timestamp\", \"\"),\n",
    "                    \"metadata\": {k: v for k, v in doc.metadata.items() \n",
    "                                if k not in [\"diagram\", \"timestamp\"]}\n",
    "                })\n",
    "                \n",
    "            logger.info(f\"Retrieved {len(diagrams)} similar diagrams from SQLite\")\n",
    "            return diagrams\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Memory retrieval failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def clear_memory(self) -> None:\n",
    "        try:\n",
    "            # Close existing connection\n",
    "            if hasattr(self, 'connection'):\n",
    "                self.connection.close()\n",
    "            \n",
    "            # Remove database file\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            \n",
    "            # Recreate connection and vector store\n",
    "            import sqlean\n",
    "            import sqlite_vec\n",
    "            \n",
    "            self.connection = sqlean.connect(self.db_path)\n",
    "            self.connection.row_factory = sqlean.Row\n",
    "            self.connection.enable_load_extension(True)\n",
    "            sqlite_vec.load(self.connection)\n",
    "            self.connection.enable_load_extension(False)\n",
    "            \n",
    "            self.vector_store = SQLiteVec(\n",
    "                table=\"uml_memories\",\n",
    "                connection=self.connection,\n",
    "                embedding=self.embedding_function\n",
    "            )\n",
    "\n",
    "            logger.info(\"Memory cleared and reinitialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to clear memory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "19a6486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_memory_from_shots(\n",
    "    memory_manager: MemoryManager,\n",
    "    shots_json_path: str = \"./../data/complete_shots.json\",\n",
    "    force_reseed: bool = False\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Seed the memory database with few-shot examples from JSON file.\n",
    "    Skips seeding if database already contains data (unless force_reseed=True).\n",
    "    \n",
    "    Args:\n",
    "        memory_manager: MemoryManager instance to seed\n",
    "        shots_json_path: Path to the complete_shots.json file\n",
    "        force_reseed: If True, clears existing data and reseeds\n",
    "        \n",
    "    Returns:\n",
    "        Number of shots seeded (0 if skipped)\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"CHECKING MEMORY SEEDING STATUS\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Check if database already has data\n",
    "    try:\n",
    "        existing_docs = memory_manager.vector_store.similarity_search(\"test\", k=1)\n",
    "        if existing_docs and not force_reseed:\n",
    "            logger.info(f\"Database already contains data ({len(existing_docs)} docs found)\")\n",
    "            logger.info(\"Skipping seeding operation. Set force_reseed=True to override.\")\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Database appears empty or uninitialized: {e}\")\n",
    "    \n",
    "    if force_reseed:\n",
    "        logger.warning(\"Force reseed enabled - clearing existing memory\")\n",
    "        memory_manager.clear_memory()\n",
    "    \n",
    "\n",
    "    if not os.path.exists(shots_json_path):\n",
    "        logger.error(f\"Shots file not found at {shots_json_path}\")\n",
    "        return 0\n",
    "    \n",
    "    logger.info(f\"Loading shots from {shots_json_path}\")\n",
    "    with open(shots_json_path, 'r', encoding='utf-8') as f:\n",
    "        shots = json.load(f)\n",
    "    \n",
    "    logger.info(f\"Found {len(shots)} shots to seed\")\n",
    "    \n",
    "    # Prepare documents\n",
    "    documents = []\n",
    "    for shot in shots:\n",
    "        requirements = shot[\"requirements\"]\n",
    "        diagram = shot[\"diagram\"]\n",
    "        \n",
    "        metadata = {\n",
    "            \"diagram\": diagram,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"plan\": shot.get(\"plan\"),\n",
    "            \"is_static\": True,\n",
    "            \"title\": shot.get(\"title\", \"Untitled\")\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"  Processing: {metadata['title']}\")\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=requirements,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    if documents:\n",
    "        memory_manager.vector_store.add_documents(documents)\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"✓ Successfully seeded {len(documents)} shots to memory\")\n",
    "        logger.info(\"=\"*60)\n",
    "        return len(documents)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UMLNodes:\n",
    "    \"\"\"\n",
    "    Collection of agent nodes for the UML generation workflow.\n",
    "    \n",
    "    Each method represents a node in the LangGraph workflow and\n",
    "    follows the pattern of taking AgentState and returning a dict\n",
    "    with state updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        plantuml_tool: PlantUMLTool,\n",
    "        memory_manager: Optional['MemoryManager'] = None,\n",
    "        config: Optional[SystemConfig] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize UML nodes with required dependencies.\n",
    "        \n",
    "        Args:\n",
    "            llm: LangChain ChatOpenAI instance\n",
    "            plantuml_tool: Tool for PlantUML validation\n",
    "            memory_manager: long-term memory manager\n",
    "            config: Optional system configuration\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.plantuml_tool = plantuml_tool\n",
    "        self.memory_manager = memory_manager\n",
    "        self.config = config or SystemConfig()\n",
    "        logger.info(\"UMLNodes initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_diagram(diagram: str) -> str:\n",
    "        \"\"\"Normalize diagram for consistent hashing (remove whitespace variations).\"\"\"\n",
    "        lines = [line.strip() for line in diagram.strip().split('\\n') if line.strip()]\n",
    "        return '\\n'.join(sorted(lines))  # Sort for order-independent comparison\n",
    "    \n",
    "    @staticmethod\n",
    "    def _hash_diagram(diagram: str) -> str:\n",
    "        \"\"\"Create a hash of the diagram content for caching.\"\"\"\n",
    "        normalized = UMLNodes._normalize_diagram(diagram)\n",
    "        return hashlib.md5(normalized.encode()).hexdigest()\n",
    "\n",
    "    def _safe_invoke(self, runnable: Any, input_data: Any, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Invoke a runnable (LLM or chain) with retry logic.\n",
    "        \"\"\"\n",
    "        max_retries = 3\n",
    "        last_exception = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return runnable.invoke(input_data, **kwargs)\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                logger.warning(f\"LLM call failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 * (attempt + 1))\n",
    "        \n",
    "        logger.error(f\"Max retries reached for LLM call: {last_exception}\")\n",
    "        raise last_exception\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_decomposition_plan(decomposition: DecompositionResult) -> str:\n",
    "        \"\"\"\n",
    "        Format a DecompositionResult into a readable plan string.\n",
    "        \n",
    "        Args:\n",
    "            decomposition: The decomposition result with classes and relationships\n",
    "            \n",
    "        Returns:\n",
    "            Formatted plan as a string\n",
    "        \"\"\"\n",
    "        lines = [\"## STRUCTURAL DECOMPOSITION\\n\"]\n",
    "        \n",
    "        # Format classes\n",
    "        if decomposition.classes:\n",
    "            lines.append(\"### Classes:\")\n",
    "            for cls in decomposition.classes:\n",
    "                attrs_str = \", \".join([f\"{attr.name}: {attr.type}\" for attr in cls.attributes])\n",
    "                lines.append(f\"- {cls.name}\" + (f\" ({attrs_str})\" if attrs_str else \"\"))\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Format relationships\n",
    "        if decomposition.relationships:\n",
    "            lines.append(\"### Relationships:\")\n",
    "            for rel in decomposition.relationships:\n",
    "                src_multiplicity_str = f\" [{rel.source_multiplicity}]\" if rel.source_multiplicity else \"\"\n",
    "                tgt_multiplicity_str = f\" [{rel.target_multiplicity}]\" if rel.target_multiplicity else \"\"\n",
    "                lines.append(f\"- {rel.source}{src_multiplicity_str} --{rel.type}--> {rel.target}{tgt_multiplicity_str}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def retrieve(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant few-shot examples based on requirements.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'examples' key containing formatted shots\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.RETRIEVE.upper()} ---\")\n",
    "        \n",
    "        try:\n",
    "            memories = self.memory_manager.retrieve_similar_diagrams(\n",
    "                state[\"requirements\"],\n",
    "                limit=self.config.num_few_shots\n",
    "            )\n",
    "            \n",
    "            formatted_shots = []\n",
    "            for mem in memories:\n",
    "                formatted_shots.append(\n",
    "                    HumanMessage(content=f\"Requirements:\\n{mem['requirements']}\")\n",
    "                )\n",
    "                \n",
    "                meta = mem.get(\"metadata\", {})\n",
    "                plan = meta.get(\"plan\", \"No plan available.\")\n",
    "                \n",
    "                assistant_content = (\n",
    "                    f\"1. DESIGN PLAN:\\n{plan}\\n\\n\"\n",
    "                    f\"2. PLANTUML DIAGRAM:\\n```plantuml\\n{mem['diagram']}\\n```\"\n",
    "                )\n",
    "                \n",
    "                formatted_shots.append(\n",
    "                    AIMessage(content=assistant_content)\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(memories)} relevant examples from unified memory\")\n",
    "            return {\"examples\": formatted_shots}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Retrieval failed: {e}\")\n",
    "            return {\"examples\": []}\n",
    "\n",
    "    def decompose(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Decompose requirements into structural building blocks.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'plan' update containing formatted decomposition\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.DECOMPOSE.upper()} ---\")\n",
    "\n",
    "        critique = state.get(\"audit_feedback\", []) or []\n",
    "        suggestions = state.get(\"audit_suggestions\", []) or []\n",
    "\n",
    "        critique_str = \"\\n\".join([f\"- {c}\" for c in critique]) if critique else \"None\"\n",
    "        suggestions_str = \"\\n\".join([f\"- {s}\" for s in suggestions]) if suggestions else \"None\"\n",
    "\n",
    "        system_prompt = DECOMPOSER_SYSTEM\n",
    "        if critique or suggestions:\n",
    "            system_prompt += (\n",
    "                \"\\n\\nIMPORTANT: You must REVISE the previous plan to address the audit.\"\n",
    "                \"\\n- Make the smallest changes needed.\"\n",
    "                \"\\n- Do not drop correct elements.\"\n",
    "                \"\\n\\nAUDIT CRITIQUE:\\n\"\n",
    "                f\"{critique_str}\"\n",
    "                \"\\n\\nAUDIT SUGGESTIONS:\\n\"\n",
    "                f\"{suggestions_str}\"\n",
    "            )\n",
    "        \n",
    "        messages = [SystemMessage(content=system_prompt)]\n",
    "        messages.append(HumanMessage(content=f\"REQUIREMENTS:\\n{state['requirements']}\"))\n",
    "\n",
    "        previous_plan = state.get(\"plan\")\n",
    "        if previous_plan:\n",
    "            messages.append(HumanMessage(content=f\"PREVIOUS PLAN (revise this):\\n{previous_plan}\"))\n",
    "        \n",
    "        try:\n",
    "            structured_llm = self.llm.bind(max_tokens=self.config.max_tokens_decompose).with_structured_output(DecompositionResult)\n",
    "            decomposition: DecompositionResult = self._safe_invoke(\n",
    "                structured_llm,\n",
    "                messages\n",
    "            )\n",
    "            logger.info(\"Decomposition completed\")\n",
    "            formatted_plan = self._format_decomposition_plan(decomposition)\n",
    "            \n",
    "            return {\"plan\": formatted_plan}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Decomposition failed: {e}\")\n",
    "            empty_decomposition = DecompositionResult(classes=[], relationships=[])\n",
    "            formatted_plan = self._format_decomposition_plan(empty_decomposition)\n",
    "            return {\"plan\": formatted_plan}\n",
    "        \n",
    "    def plan_auditor(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Audits the structural plan for logical consistency and requirement coverage.\n",
    "        \n",
    "        Uses PlanAudit Pydantic model for structured output validation.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'plan_valid' and 'audit_feedback' updates\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.PLAN_AUDIT.upper()} ---\")\n",
    "        \n",
    "        # Check if we've exceeded max plan audit attempts\n",
    "        max_plan_audit_attempts = 3\n",
    "        current_attempts = state.get(\"plan_audit_attempts\", 0) + 1\n",
    "        \n",
    "        if current_attempts > max_plan_audit_attempts:\n",
    "            logger.warning(f\"Max plan audit attempts ({max_plan_audit_attempts}) reached. Forcing plan validation.\")\n",
    "            return {\n",
    "                \"plan_valid\": True,\n",
    "                \"audit_feedback\": [],\n",
    "                \"audit_suggestions\": [],\n",
    "                \"plan_audit_attempts\": current_attempts\n",
    "            }\n",
    "        \n",
    "        plan = state.get(\"plan\")\n",
    "        requirements = state.get(\"requirements\")\n",
    "        req_len = len(requirements) if isinstance(requirements, str) else 0\n",
    "        plan_len = len(plan) if isinstance(plan, str) else 0\n",
    "        logger.debug(f\"Plan audit inputs: requirements_len={req_len}, plan_len={plan_len}\")\n",
    "        logger.debug(f\"Plan audit prompt template chars={len(PLAN_AUDITOR_SYSTEM)}\")\n",
    "        if not requirements:\n",
    "            logger.warning(\"Plan audit received empty requirements\")\n",
    "        if not plan:\n",
    "            logger.warning(\"Plan audit received empty plan\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        REQUIREMENTS:\n",
    "        {requirements}\n",
    "        \n",
    "        PROPOSED PLAN:\n",
    "        {plan}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            messages = [\n",
    "                SystemMessage(content=PLAN_AUDITOR_SYSTEM),\n",
    "                HumanMessage(content=prompt)\n",
    "            ]\n",
    "            # Questa sezione di codice si può probabilmente semplificare\n",
    "            try:\n",
    "                structured_llm = self.llm.with_structured_output(PlanAudit, include_raw=True)\n",
    "                audit_payload = structured_llm.invoke(messages)\n",
    "                audit_result: Optional[PlanAudit] = audit_payload.get(\"parsed\")\n",
    "                raw_msg = audit_payload.get(\"raw\")\n",
    "                parsing_error = audit_payload.get(\"parsing_error\")\n",
    "                if raw_msg is not None and getattr(raw_msg, 'content', None) is not None:\n",
    "                    raw_text = str(raw_msg.content)\n",
    "                    raw_preview = raw_text[:2000] + (\"...\" if len(raw_text) > 2000 else \"\")\n",
    "                    logger.debug(f\"Plan audit raw response (preview): {raw_preview}\")\n",
    "                if parsing_error is not None:\n",
    "                    logger.warning(f\"Plan audit parsing_error: {parsing_error}\")\n",
    "                if audit_result is None:\n",
    "                    raise ValueError(\"Plan audit produced no parsed result\")\n",
    "            except TypeError:\n",
    "                audit_result = self.llm.with_structured_output(PlanAudit).invoke(messages)\n",
    "            elapsed = time.perf_counter() - t0\n",
    "            logger.debug(f\"Plan audit invoke took {elapsed:.2f}s\")\n",
    "            \n",
    "            logger.info(f\"Plan audit completed: valid={audit_result.is_valid}\")\n",
    "            logger.debug(f\"Plan audit parsed: critique_count={len(audit_result.critique)}, suggestions_count={len(audit_result.suggestions)}\")\n",
    "            \n",
    "            logger.info(f\"Output from plan auditor: {audit_result.model_dump()}\")\n",
    "            \n",
    "            #if not audit_result.is_valid:\n",
    "            if audit_result.critique:\n",
    "                logger.info(f\"Audit issues (first 5): {', '.join(audit_result.critique[:5])}\")\n",
    "            if audit_result.suggestions:\n",
    "                logger.info(f\"Audit suggestions (first 5): {', '.join(audit_result.suggestions[:5])}\")\n",
    "            logger.debug(f\"Plan audit full result: {audit_result.model_dump()}\")\n",
    "            \n",
    "            return {\n",
    "                \"plan_valid\": audit_result.is_valid,\n",
    "                \"audit_feedback\": audit_result.critique,\n",
    "                \"audit_suggestions\": audit_result.suggestions,\n",
    "                \"plan_audit_attempts\": current_attempts\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Plan audit failed: {e}\")\n",
    "            return {\n",
    "                \"plan_valid\": False,\n",
    "                \"audit_feedback\": [f\"Audit error: {str(e)}\"],\n",
    "                \"audit_suggestions\": [],\n",
    "                \"plan_audit_attempts\": current_attempts  \n",
    "            }\n",
    "\n",
    "    def generate(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate PlantUML diagram using chain-of-thought reasoning.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'current_diagram' and 'iterations' updates\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.GENERATE.upper()} ---\")\n",
    "        \n",
    "        messages = [SystemMessage(content=GENERATOR_SYSTEM)]\n",
    "        \n",
    "        # Add few-shot examples if available\n",
    "        if state.get(\"examples\"):\n",
    "            messages.extend(state[\"examples\"])\n",
    "            logger.debug(f\"Added {len(state['examples'])} example messages\")\n",
    "            \n",
    "        user_content = f\"\"\"\n",
    "        # VALIDATED REQUIREMENTS\n",
    "        {state['requirements']}\n",
    "\n",
    "        # VALIDATED DESIGN PLAN\n",
    "        {state['plan']}\n",
    "\n",
    "        # TASK\n",
    "        Render the PlantUML class diagram exactly from the design plan.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add syntax error feedback if we're retrying after syntax check failure\n",
    "        if not state.get(\"syntax_valid\", True) and state.get(\"error_message\"):\n",
    "            user_content += f\"\"\"\n",
    "        \n",
    "        # PREVIOUS ATTEMPT HAD SYNTAX ERROR\n",
    "        {state['error_message']}\n",
    "        \n",
    "        Fix the syntax error and regenerate the diagram.\n",
    "        \"\"\"\n",
    "            logger.info(f\"Added syntax error feedback to generation prompt\")\n",
    "        \n",
    "        messages.append(HumanMessage(content=user_content))\n",
    "\n",
    "        logger.debug(f\"Generation prompt messages: {messages}\")\n",
    "        \n",
    "        try:\n",
    "            response = self._safe_invoke(\n",
    "                self.llm,\n",
    "                messages,\n",
    "                max_tokens=self.config.max_tokens_generate\n",
    "            )\n",
    "            diagram = self.plantuml_tool.extract_plantuml(response.content)\n",
    "            \n",
    "            logger.info(f\"Generation completed (iteration {state['iterations'] + 1})\")\n",
    "            return {\n",
    "                \"current_diagram\": diagram,\n",
    "                \"iterations\": state[\"iterations\"] + 1,\n",
    "                \"stagnant_count\": 0  # Reset stagnation counter on new generation\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "            return {\n",
    "                \"current_diagram\": f\"Error: {str(e)}\",\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }\n",
    "\n",
    "    def syntax_check(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate PlantUML syntax through server.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'syntax_valid' and optional 'error_message'\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.SYNTAX_CHECK.upper()} ---\")\n",
    "        \n",
    "        try:\n",
    "            result = self.plantuml_tool.check_syntax(\n",
    "                state[\"current_diagram\"],\n",
    "                timeout=self.config.request_timeout\n",
    "            )\n",
    "            \n",
    "            if result.is_valid:\n",
    "                logger.info(f\"Syntax valid. View at: {result.url}\")\n",
    "            else:\n",
    "                logger.warning(f\"Syntax error: {result.error}\")\n",
    "            \n",
    "            return {\n",
    "                \"syntax_valid\": result.is_valid,\n",
    "                \"error_message\": result.error if not result.is_valid else None,\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Syntax check failed: {e}\")\n",
    "            return {\n",
    "                \"syntax_valid\": False,\n",
    "                \"error_message\": f\"Syntax check error: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    def critic(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Critic node: produces a formal CritiqueReport, tracks semantic progress,\n",
    "        and updates stagnation metrics.\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.CRITIC.upper()} ---\")\n",
    "\n",
    "        try:\n",
    "            requirements = state[\"requirements\"]\n",
    "            diagram = self.plantuml_tool.extract_plantuml(state[\"current_diagram\"])\n",
    "\n",
    "            messages = [\n",
    "                SystemMessage(content=CRITIC_SYSTEM),\n",
    "                HumanMessage(content=json.dumps({\n",
    "                    \"requirements\": requirements,\n",
    "                    \"diagram\": diagram\n",
    "                }))\n",
    "            ]\n",
    "\n",
    "            structured_llm = self.llm.bind(max_tokens=self.config.max_tokens_critique).with_structured_output(CritiqueReport)\n",
    "            report: CritiqueReport = self._safe_invoke(structured_llm, messages)\n",
    "\n",
    "            # ---- STAGNATION TRACKING ----\n",
    "            prev_ids = set(state.get(\"previous_finding_ids\", []))\n",
    "            curr_ids = {f.id for f in report.findings}\n",
    "\n",
    "            resolved = prev_ids - curr_ids\n",
    "            new = curr_ids - prev_ids\n",
    "\n",
    "            if not resolved and not new:\n",
    "                stagnant_count = state.get(\"stagnant_count\", 0) + 1\n",
    "            else:\n",
    "                stagnant_count = 0\n",
    "\n",
    "            # ---- UPDATE SUMMARY ----\n",
    "            summary: CritiqueSummary = CritiqueSummary(\n",
    "                total_findings=len(report.findings),\n",
    "                render_only=sum(1 for f in report.findings if f.fixability == Fixability.render_only),\n",
    "                structure_change=sum(1 for f in report.findings if f.fixability == Fixability.structure_change),\n",
    "                unfixable=sum(1 for f in report.findings if f.fixability == Fixability.unfixable),\n",
    "                new_findings=len(new),\n",
    "                resolved_findings=len(resolved)\n",
    "            )\n",
    "\n",
    "            logger.info(\n",
    "                f\"Critic findings: total={report.summary.total_findings}, \"\n",
    "                f\"render_only={summary.render_only}, \"\n",
    "                f\"structural={summary.structure_change}, \"\n",
    "                f\"unfixable={summary.unfixable}, \"\n",
    "                f\"new={summary.new_findings}, resolved={summary.resolved_findings}\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"current_validation\": report,\n",
    "                \"previous_finding_ids\": list(curr_ids),\n",
    "                \"stagnant_count\": stagnant_count,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critic node failed: {e}\")\n",
    "            return {\n",
    "                \"logic_valid\": False,\n",
    "                \"current_validation\": None,\n",
    "                \"error_message\": str(e)\n",
    "            }\n",
    "\n",
    "    def reflector(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Reflector node: fixes render-only issues in the current PlantUML diagram\n",
    "        based on structured critique findings.\n",
    "\n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "\n",
    "        Returns:\n",
    "            Dict with updated current_diagram and iterations\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.REFLECTOR.upper()} ---\")\n",
    "\n",
    "        try:\n",
    "            # Extract findings flagged as render-only from current_validation\n",
    "            current_validation = state.get(\"current_validation\")\n",
    "            findings = current_validation.findings if current_validation else []\n",
    "            renderable_findings = [\n",
    "                f for f in findings if f.fixability == Fixability.render_only\n",
    "            ]\n",
    "\n",
    "            # If no render-only findings, also check for errors to provide context\n",
    "            if not renderable_findings:\n",
    "                logger.info(\"No render-only issues to fix. Returning current diagram unchanged.\")\n",
    "                return {\n",
    "                    \"current_diagram\": state[\"current_diagram\"],\n",
    "                    \"iterations\": state[\"iterations\"] + 1,\n",
    "                }\n",
    "\n",
    "            reflector_input = {\n",
    "                \"diagram\": state[\"current_diagram\"],\n",
    "                \"findings\": [f.model_dump() for f in renderable_findings],\n",
    "            }\n",
    "\n",
    "            system_msg = SystemMessage(content=REFLECTOR_SYSTEM)\n",
    "            user_msg = HumanMessage(content=json.dumps(reflector_input))\n",
    "\n",
    "            reflected_diagram_response = self._safe_invoke(\n",
    "                self.llm,\n",
    "                [system_msg, user_msg],\n",
    "                max_tokens=self.config.max_tokens_reflect\n",
    "            )\n",
    "\n",
    "            reflected_diagram = self.plantuml_tool.extract_plantuml(reflected_diagram_response.content)\n",
    "\n",
    "            logger.info(f\"Reflector applied fixes for {len(reflector_input['findings'])} issues.\")\n",
    "\n",
    "            return {\n",
    "                \"current_diagram\": reflected_diagram,\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reflector node failed: {e}\")\n",
    "            return {\n",
    "                \"current_diagram\": state.get(\"current_diagram\"),\n",
    "                \"error_message\": str(e),\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }\n",
    "    \n",
    "    def structure_refiner(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Structure Refiner node: applies guided structural fixes using\n",
    "        expected_correction from critique findings.\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- NODE: {NodeNames.STRUCTURE_REFINER.upper()} ---\")\n",
    "\n",
    "        try:\n",
    "            validation = state.get(\"current_validation\")\n",
    "            if not validation:\n",
    "                logger.warning(\"No validation report. Structure refiner is a no-op.\")\n",
    "                return {\"current_diagram\": state[\"current_diagram\"]}\n",
    "\n",
    "            findings = [\n",
    "                f for f in validation.findings\n",
    "                if f.fixability == Fixability.structure_change\n",
    "            ]\n",
    "\n",
    "            # Safety: every structural finding must have an expected correction\n",
    "            for f in findings:\n",
    "                if not f.expected_correction:\n",
    "                    logger.error(\n",
    "                        f\"Structural finding missing expected_correction: {f.id}\"\n",
    "                    )\n",
    "                    return {\n",
    "                        \"current_diagram\": state[\"current_diagram\"],\n",
    "                        \"error_message\": f\"Unfixable structural finding: {f.id}\"\n",
    "                    }\n",
    "\n",
    "            if not findings:\n",
    "                logger.info(\"No structural findings to apply.\")\n",
    "                return {\"current_diagram\": state[\"current_diagram\"]}\n",
    "\n",
    "            refiner_input = {\n",
    "                \"requirements\": state[\"requirements\"],\n",
    "                \"diagram\": state[\"current_diagram\"],\n",
    "                \"findings\": [f.model_dump() for f in findings]\n",
    "            }\n",
    "\n",
    "            messages = [\n",
    "                SystemMessage(content=STRUCTURE_REFINER_SYSTEM),\n",
    "                HumanMessage(content=json.dumps(refiner_input))\n",
    "            ]\n",
    "\n",
    "            response = self._safe_invoke(\n",
    "                self.llm,\n",
    "                messages,\n",
    "                max_tokens=self.config.max_tokens_refine\n",
    "            )\n",
    "\n",
    "            refined_diagram = self.plantuml_tool.extract_plantuml(response.content)\n",
    "\n",
    "            logger.info(\n",
    "                f\"Structure refiner applied {len(findings)} guided corrections.\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"current_diagram\": refined_diagram,\n",
    "                \"iterations\": state[\"iterations\"] + 1\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Structure refiner failed: {e}\")\n",
    "            return {\n",
    "                \"current_diagram\": state.get(\"current_diagram\"),\n",
    "                \"error_message\": str(e)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "1fdc4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_iteration_metrics(state: AgentState, cfg: SystemConfig) -> None:\n",
    "    \"\"\"\n",
    "    Update iteration-based metrics for stopping conditions:\n",
    "    - Weighted score delta\n",
    "    - Plateau detection\n",
    "    - Stagnant iteration count\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state\n",
    "        cfg: System configuration (plateau_window, plateau_threshold)\n",
    "    \"\"\"\n",
    "    current_validation = state.get(\"current_validation\")\n",
    "    if not current_validation:\n",
    "        state[\"delta_score\"] = 0.0\n",
    "        state[\"stagnant_count\"] = state.get(\"stagnant_count\", 0)\n",
    "        return\n",
    "\n",
    "    current_score = getattr(current_validation, \"weighted_score\", 0.0)\n",
    "    state.setdefault(\"score_history\", []).insert(0, current_score)  # newest first\n",
    "\n",
    "    # Limit history to plateau window\n",
    "    state[\"score_history\"] = state[\"score_history\"][:cfg.plateau_window]\n",
    "\n",
    "    # Compute delta from previous iteration\n",
    "    if len(state[\"score_history\"]) > 1:\n",
    "        delta = current_score - state[\"score_history\"][1]\n",
    "    else:\n",
    "        delta = current_score  # first iteration\n",
    "\n",
    "    state[\"delta_score\"] = delta\n",
    "\n",
    "    # Check for plateau\n",
    "    plateau_detected = False\n",
    "    if len(state[\"score_history\"]) >= cfg.plateau_window:\n",
    "        deltas = [\n",
    "            abs(state[\"score_history\"][i] - state[\"score_history\"][i+1])\n",
    "            for i in range(len(state[\"score_history\"]) - 1)\n",
    "        ]\n",
    "        if all(d < cfg.plateau_threshold for d in deltas):\n",
    "            plateau_detected = True\n",
    "\n",
    "    # Update stagnant count\n",
    "    if plateau_detected:\n",
    "        state[\"stagnant_count\"] = state.get(\"stagnant_count\", 0) + 1\n",
    "    else:\n",
    "        state[\"stagnant_count\"] = 0\n",
    "\n",
    "    logger.debug(\n",
    "        f\"Iteration metrics updated: current_score={current_score:.2f}, \"\n",
    "        f\"delta_score={delta:.2f}, stagnant_count={state['stagnant_count']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12124ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uml_graph(\n",
    "    nodes: UMLNodes, \n",
    "    config: Optional[SystemConfig] = None\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Create the LangGraph workflow for UML diagram generation.\n",
    "    \n",
    "    Args:\n",
    "        nodes: UMLNodes instance with all agent methods\n",
    "        config: Optional system configuration\n",
    "        \n",
    "    Returns:\n",
    "        Compiled LangGraph workflow\n",
    "    \"\"\"\n",
    "    cfg = config or SystemConfig()\n",
    "    logger.info(\"Creating UML generation workflow\")\n",
    "    \n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Add all nodes\n",
    "    workflow.add_node(NodeNames.RETRIEVE, nodes.retrieve)\n",
    "    workflow.add_node(NodeNames.PLAN_AUDIT, nodes.plan_auditor)\n",
    "    workflow.add_node(NodeNames.DECOMPOSE, nodes.decompose)\n",
    "    workflow.add_node(NodeNames.GENERATE, nodes.generate)\n",
    "    workflow.add_node(NodeNames.SYNTAX_CHECK, nodes.syntax_check)\n",
    "    workflow.add_node(NodeNames.CRITIC, nodes.critic)\n",
    "    workflow.add_node(NodeNames.REFLECTOR, nodes.reflector)\n",
    "    workflow.add_node(NodeNames.STRUCTURE_REFINER, nodes.structure_refiner)\n",
    "    \n",
    "    logger.debug(\"Added 8 nodes to workflow\")\n",
    "\n",
    "    # Define edges\n",
    "    workflow.add_edge(START, NodeNames.RETRIEVE)\n",
    "    workflow.add_edge(NodeNames.RETRIEVE, NodeNames.DECOMPOSE)\n",
    "    workflow.add_edge(NodeNames.DECOMPOSE, NodeNames.PLAN_AUDIT)\n",
    "\n",
    "    def route_after_plan_audit(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Route based on plan audit results.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Next node name\n",
    "        \"\"\"\n",
    "        if state.get(\"plan_valid\", False):\n",
    "            logger.debug(\"Routing: plan_audit -> generate\")\n",
    "            return NodeNames.GENERATE\n",
    "            \n",
    "        logger.debug(\"Routing: plan_audit -> decompose\")\n",
    "        return NodeNames.DECOMPOSE\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.PLAN_AUDIT, \n",
    "        route_after_plan_audit,\n",
    "        {\n",
    "            NodeNames.DECOMPOSE: NodeNames.DECOMPOSE,\n",
    "            NodeNames.GENERATE: NodeNames.GENERATE\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(NodeNames.GENERATE, NodeNames.SYNTAX_CHECK)\n",
    "\n",
    "    def route_after_syntax_check(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Route based on syntax validation results and iteration limits.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Next node name\n",
    "        \"\"\"\n",
    "        if state.get(\"syntax_valid\", False):\n",
    "            logger.debug(\"Routing: syntax_check -> critic\")\n",
    "            return NodeNames.CRITIC\n",
    "            \n",
    "        if state[\"iterations\"] >= cfg.max_iterations:\n",
    "            logger.warning(f\"Max iterations ({cfg.max_iterations}) reached during syntax check\")\n",
    "            return END\n",
    "        \n",
    "        logger.debug(\"Routing: syntax_check -> generate (syntax error)\")\n",
    "        return NodeNames.GENERATE\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.SYNTAX_CHECK, \n",
    "        route_after_syntax_check,\n",
    "        {\n",
    "            NodeNames.CRITIC: NodeNames.CRITIC,\n",
    "            NodeNames.GENERATE: NodeNames.GENERATE,\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def route_after_critic(state: AgentState) -> str:\n",
    "        update_iteration_metrics(state, cfg)  # optional metric tracking\n",
    "\n",
    "        validation = state.get(\"current_validation\")\n",
    "        if not validation:\n",
    "            logger.error(\"No critic report; stopping workflow.\")\n",
    "            return END\n",
    "\n",
    "        summary = validation.summary\n",
    "\n",
    "        # Step 1: Fully valid diagram\n",
    "        if validation.is_valid:\n",
    "            logger.info(\"CRITIC passed: diagram is fully valid → END\")\n",
    "            return END\n",
    "\n",
    "        # Step 2: Check for unfixable structural issues\n",
    "        if summary.unfixable > 0:\n",
    "            logger.warning(\"CRITIC detected unfixable issues → END\")\n",
    "            return END\n",
    "\n",
    "        # Step 3: Check for stagnation\n",
    "        if state.get(\"stagnant_count\", 0) >= cfg.max_stagnant_iterations:\n",
    "            logger.warning(f\"Stagnation detected ({cfg.max_stagnant_iterations}) → END\")\n",
    "            return END\n",
    "\n",
    "        # Step 4: Structural corrections required\n",
    "        if summary.structure_change > 0:\n",
    "            logger.debug(\"Routing: CRITIC → STRUCTURE_REFINER (structural issues exist)\")\n",
    "            return NodeNames.STRUCTURE_REFINER\n",
    "\n",
    "        # Step 5: Render-only issues can be fixed\n",
    "        if summary.render_only > 0:\n",
    "            logger.debug(\"Routing: CRITIC → REFLECTOR (render-only issues exist)\")\n",
    "            return NodeNames.REFLECTOR\n",
    "\n",
    "        # Safety net\n",
    "        logger.warning(\"CRITIC findings not actionable; ending workflow\")\n",
    "        return END\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.CRITIC,\n",
    "        route_after_critic,\n",
    "        {\n",
    "            NodeNames.REFLECTOR: NodeNames.REFLECTOR,\n",
    "            NodeNames.STRUCTURE_REFINER: NodeNames.STRUCTURE_REFINER,\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    def route_after_reflector(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Route after reflection based on iteration limits and stagnation.\n",
    "        \n",
    "        Args:\n",
    "            state: Current workflow state\n",
    "            \n",
    "        Returns:\n",
    "            Next node name\n",
    "        \"\"\"\n",
    "        if state[\"iterations\"] >= cfg.max_iterations:\n",
    "            logger.warning(f\"Max iterations ({cfg.max_iterations}) reached after reflection\")\n",
    "            return END\n",
    "        logger.debug(\"Routing: reflect -> syntax_check\")\n",
    "        return NodeNames.SYNTAX_CHECK\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.REFLECTOR, \n",
    "        route_after_reflector,\n",
    "        {\n",
    "            NodeNames.SYNTAX_CHECK: NodeNames.SYNTAX_CHECK,\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def route_after_structure_refiner(state: AgentState) -> str:\n",
    "        if state[\"iterations\"] >= cfg.max_iterations:\n",
    "            logger.warning(f\"Max iterations ({cfg.max_iterations}) reached after STRUCTURE_REFINER\")\n",
    "            return END\n",
    "\n",
    "        # After structural corrections, syntax check first\n",
    "        logger.debug(\"Routing: STRUCTURE_REFINER → SYNTAX_CHECK\")\n",
    "        return NodeNames.SYNTAX_CHECK\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        NodeNames.STRUCTURE_REFINER,\n",
    "        route_after_structure_refiner,\n",
    "        {\n",
    "            NodeNames.SYNTAX_CHECK: NodeNames.SYNTAX_CHECK, \n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "        \n",
    "    logger.info(\"Workflow graph created successfully\")\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "def create_initial_state(requirements: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Create an initial state for the workflow.\n",
    "    \n",
    "    Args:\n",
    "        requirements: Software requirements text\n",
    "        \n",
    "    Returns:\n",
    "        Initial AgentState dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"requirements\": requirements,\n",
    "        \"plan\": None,\n",
    "        \"examples\": [],\n",
    "        \"current_diagram\": None,\n",
    "        \"best_diagram\": None,\n",
    "        \"summary\": None,\n",
    "        \"syntax_valid\": False,\n",
    "        \"logic_valid\": False,\n",
    "        \"iterations\": 0,\n",
    "        \"error_message\": None,\n",
    "        \"failed_attempts\": [],\n",
    "        \"stagnant_count\": 0,\n",
    "        \"critique_cache\": {},\n",
    "        # Additional required fields\n",
    "        \"plan_valid\": False,\n",
    "        \"audit_feedback\": None,\n",
    "        \"plan_audit_attempts\": 0,\n",
    "        \"best_score\": 0.0,\n",
    "        \"best_code\": \"\",\n",
    "        \"current_validation\": None,\n",
    "        \"score_history\": [],\n",
    "        \"delta_score\": 0.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "c3657577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 01:53:34,539 - __main__ - INFO - Loading test exercises from ./../data/test_exercises.json\n",
      "2026-01-16 01:53:34,541 - __main__ - INFO - Loaded 8 test exercises\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 test exercises\n"
     ]
    }
   ],
   "source": [
    "def load_test_exercises(json_path: str = \"./../data/test_exercises.json\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load test exercises from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to test exercises JSON\n",
    "        \n",
    "    Returns:\n",
    "        List of exercise dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If file doesn't exist\n",
    "        json.JSONDecodeError: If JSON is invalid\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading test exercises from {json_path}\")\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"Test exercises file not found: {json_path}\")\n",
    "    \n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        exercises = json.load(f)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(exercises)} test exercises\")\n",
    "    return exercises\n",
    "\n",
    "try:\n",
    "    test_exercises = load_test_exercises()\n",
    "    print(f\"Loaded {len(test_exercises)} test exercises\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load test exercises: {e}\")\n",
    "    test_exercises = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "e53188a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 01:53:34,551 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:53:34,551 - __main__ - INFO - INITIALIZING UML GENERATION SYSTEM\n",
      "2026-01-16 01:53:34,552 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:53:34,552 - __main__ - INFO - Creating LLM connection...\n",
      "2026-01-16 01:53:34,552 - __main__ - INFO - Connecting to LMStudio at http://localhost:1234/v1\n",
      "2026-01-16 01:53:34,552 - __main__ - INFO - Using model: mistralai/devstral-small-2-2512 (temp=0.15)\n",
      "2026-01-16 01:53:34,556 - __main__ - INFO - Initializing PlantUML tool...\n",
      "2026-01-16 01:53:34,556 - __main__ - INFO - PlantUML tool initialized with host: http://localhost:8080\n",
      "2026-01-16 01:53:34,556 - __main__ - INFO - Initializing long-term memory with BAAI/bge-large-en-v1.5...\n",
      "2026-01-16 01:53:34,560 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps\n",
      "2026-01-16 01:53:34,561 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "2026-01-16 01:53:34,923 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:34,973 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:35,162 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:35,229 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:35,404 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:35,485 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:35,683 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:35,764 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:35,951 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:36,014 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:36,196 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:36,324 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:36,522 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-16 01:53:36,729 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:36,841 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6190a3c25feb4c1eb5edbc6920bd9300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-16 01:53:37,288 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:37,364 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:37,563 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-16 01:53:37,766 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:38,081 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:53:38,133 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:53:38,338 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5 \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:53,671 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n",
      "2026-01-16 01:54:54,328 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:54,390 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:54,593 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:54,671 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:54,859 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:54,956 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:55,162 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:55,245 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:55,444 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:55,535 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:55,715 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:55,811 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:55,995 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-16 01:54:56,206 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:56,290 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ffd1a8c8e6445db02d7a88ce1fee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: BAAI/bge-large-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-16 01:54:57,010 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:57,084 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:57,282 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-16 01:54:57,510 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:58,322 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-16 01:54:58,392 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/BAAI/bge-large-en-v1.5/d4aa6901d3a41ba39fb536a557fa166f842b0e09/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:58,621 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/BAAI/bge-large-en-v1.5 \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:54:58,955 - __main__ - INFO - Used sqlean for SQLite connection (extension support enabled)\n",
      "2026-01-16 01:55:02,181 - __main__ - INFO - MemoryManager initialized with LangChain SQLiteVec at ./../data/uml_knowledge.db (dims=1024)\n",
      "2026-01-16 01:55:02,183 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:02,183 - __main__ - INFO - CHECKING MEMORY SEEDING STATUS\n",
      "2026-01-16 01:55:02,183 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:02,352 - __main__ - WARNING - Force reseed enabled - clearing existing memory\n",
      "2026-01-16 01:55:02,454 - __main__ - INFO - Memory cleared and reinitialized\n",
      "2026-01-16 01:55:02,455 - __main__ - INFO - Loading shots from ./../data/complete_shots.json\n",
      "2026-01-16 01:55:02,456 - __main__ - INFO - Found 20 shots to seed\n",
      "2026-01-16 01:55:02,456 - __main__ - INFO -   Processing: Project Management System\n",
      "2026-01-16 01:55:02,456 - __main__ - INFO -   Processing: Hollywood Approach\n",
      "2026-01-16 01:55:02,457 - __main__ - INFO -   Processing: Word Processor\n",
      "2026-01-16 01:55:02,457 - __main__ - INFO -   Processing: Patient Record and Scheduling\n",
      "2026-01-16 01:55:02,457 - __main__ - INFO -   Processing: Movie-Shop\n",
      "2026-01-16 01:55:02,457 - __main__ - INFO -   Processing: Flights\n",
      "2026-01-16 01:55:02,458 - __main__ - INFO -   Processing: Bank System\n",
      "2026-01-16 01:55:02,458 - __main__ - INFO -   Processing: Veterinary Clinic\n",
      "2026-01-16 01:55:02,458 - __main__ - INFO -   Processing: Auto Repair\n",
      "2026-01-16 01:55:02,458 - __main__ - INFO -   Processing: Restaurant\n",
      "2026-01-16 01:55:02,458 - __main__ - INFO -   Processing: Deliveries\n",
      "2026-01-16 01:55:02,459 - __main__ - INFO -   Processing: Furniture Factory Management\n",
      "2026-01-16 01:55:02,459 - __main__ - INFO -   Processing: Industrial Factory Operations\n",
      "2026-01-16 01:55:02,459 - __main__ - INFO -   Processing: Bycicle Rental\n",
      "2026-01-16 01:55:02,459 - __main__ - INFO -   Processing: Car Park Access System\n",
      "2026-01-16 01:55:02,459 - __main__ - INFO -   Processing: Banking Organizational Structure\n",
      "2026-01-16 01:55:02,460 - __main__ - INFO -   Processing: Prepaid Cell Phone (Decorator Pattern)\n",
      "2026-01-16 01:55:02,460 - __main__ - INFO -   Processing: Library Management System\n",
      "2026-01-16 01:55:02,460 - __main__ - INFO -   Processing: MyDoctor Appointment Management\n",
      "2026-01-16 01:55:02,460 - __main__ - INFO -   Processing: Online Shopping System\n",
      "2026-01-16 01:55:09,600 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:09,605 - __main__ - INFO - ✓ Successfully seeded 20 shots to memory\n",
      "2026-01-16 01:55:09,605 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:09,605 - __main__ - INFO - Long-term memory (SQLite + sqlite-vec) enabled\n",
      "2026-01-16 01:55:09,606 - __main__ - INFO - Seeded 20 few-shot examples into memory\n",
      "2026-01-16 01:55:09,606 - __main__ - INFO - Building LangGraph workflow...\n",
      "2026-01-16 01:55:09,607 - __main__ - INFO - UMLNodes initialized\n",
      "2026-01-16 01:55:09,607 - __main__ - INFO - Creating UML generation workflow\n",
      "2026-01-16 01:55:09,678 - __main__ - INFO - Workflow graph created successfully\n",
      "2026-01-16 01:55:09,715 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:09,715 - __main__ - INFO - SYSTEM INITIALIZED SUCCESSFULLY\n",
      "2026-01-16 01:55:09,715 - __main__ - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System ready for diagram generation\n",
      "Long-term memory: ENABLED\n"
     ]
    }
   ],
   "source": [
    "def initialize_system(\n",
    "    config: Optional[SystemConfig] = None,\n",
    "    enable_long_term_memory: bool = True\n",
    ") -> Tuple[UMLNodes, Any, SystemConfig, Optional[MemoryManager]]:\n",
    "    \"\"\"\n",
    "    Initialize all system components.\n",
    "    \n",
    "    Args:\n",
    "        config: Optional system configuration\n",
    "        enable_long_term_memory: Whether to enable long-term memory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (nodes, compiled_workflow, config, memory_manager)\n",
    "    \"\"\"\n",
    "    cfg = config or SystemConfig()\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"INITIALIZING UML GENERATION SYSTEM\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Creating LLM connection...\")\n",
    "        llm = create_llm(cfg)\n",
    "        \n",
    "        logger.info(\"Initializing PlantUML tool...\")\n",
    "        puml_tool = PlantUMLTool(cfg.plantuml_host)\n",
    "        \n",
    "        memory_mgr = None\n",
    "        if enable_long_term_memory:\n",
    "            logger.info(f\"Initializing long-term memory with {cfg.embedder_model}...\")\n",
    "\n",
    "            dims = 1024 if \"large\" in cfg.embedder_model.lower() else 384\n",
    "            \n",
    "            memory_mgr = MemoryManager(\n",
    "                embedder=SentenceTransformer(cfg.embedder_model),\n",
    "                db_path=cfg.db_path,\n",
    "                embedding_dims=dims\n",
    "            )\n",
    "\n",
    "            seeded_count = seed_memory_from_shots(\n",
    "                memory_manager=memory_mgr,\n",
    "                shots_json_path=cfg.shots_json_path,\n",
    "                force_reseed=True \n",
    "            )\n",
    "\n",
    "            logger.info(\"Long-term memory (SQLite + sqlite-vec) enabled\")\n",
    "\n",
    "            if seeded_count > 0:\n",
    "                logger.info(f\"Seeded {seeded_count} few-shot examples into memory\")\n",
    "        else:\n",
    "            logger.info(\"Long-term memory disabled\")\n",
    "        \n",
    "\n",
    "        logger.info(\"Building LangGraph workflow...\")\n",
    "        nodes = UMLNodes(llm, puml_tool, memory_mgr, cfg)\n",
    "        app = create_uml_graph(nodes, cfg)\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"SYSTEM INITIALIZED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        return nodes, app, cfg, memory_mgr\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"System initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "nodes, app, config, memory_manager = initialize_system(enable_long_term_memory=True)\n",
    "print(\"\\nSystem ready for diagram generation\")\n",
    "print(f\"Long-term memory: {'ENABLED' if memory_manager else 'DISABLED'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "99f339c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 01:55:09,784 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:09,785 - __main__ - INFO - RUNNING: Exercise 3\n",
      "2026-01-16 01:55:09,785 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:55:09,786 - __main__ - INFO - Requirements preview: A library system manages books, members, and loans.\n",
      "Books have an ISBN, title, author, publisher, publication year, and availability status.\n",
      "Members h...\n",
      "2026-01-16 01:55:09,800 - __main__ - INFO - --- NODE: RETRIEVE ---\n",
      "2026-01-16 01:55:13,315 - __main__ - INFO - Retrieved 3 similar diagrams from SQLite\n",
      "2026-01-16 01:55:13,321 - __main__ - INFO - Retrieved 3 relevant examples from unified memory\n",
      "2026-01-16 01:55:13,324 - __main__ - INFO - --- NODE: DECOMPOSE ---\n",
      "2026-01-16 01:56:26,819 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:56:26,862 - __main__ - INFO - Decomposition completed\n",
      "2026-01-16 01:56:26,864 - __main__ - INFO - --- NODE: PLAN_AUDIT ---\n",
      "2026-01-16 01:56:40,244 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:56:40,272 - __main__ - INFO - Plan audit completed: valid=True\n",
      "2026-01-16 01:56:40,272 - __main__ - INFO - Output from plan auditor: {'critique': [], 'suggestions': [], 'is_valid': True}\n",
      "2026-01-16 01:56:40,273 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-16 01:57:40,936 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:57:40,949 - __main__ - INFO - Generation completed (iteration 1)\n",
      "2026-01-16 01:57:40,955 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-16 01:57:40,957 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-16 01:57:41,173 - __main__ - WARNING - PNG rendering failed. Fetching detailed syntax error...\n",
      "2026-01-16 01:57:41,190 - __main__ - ERROR - Syntax error detected: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:57:41,192 - __main__ - WARNING - Syntax error: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:57:41,193 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-16 01:57:41,194 - __main__ - INFO - Added syntax error feedback to generation prompt\n",
      "2026-01-16 01:58:04,792 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:58:04,795 - __main__ - INFO - Generation completed (iteration 2)\n",
      "2026-01-16 01:58:04,797 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-16 01:58:04,799 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-16 01:58:04,860 - __main__ - WARNING - PNG rendering failed. Fetching detailed syntax error...\n",
      "2026-01-16 01:58:04,874 - __main__ - ERROR - Syntax error detected: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:58:04,875 - __main__ - WARNING - Syntax error: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:58:04,876 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-16 01:58:04,876 - __main__ - INFO - Added syntax error feedback to generation prompt\n",
      "2026-01-16 01:58:26,581 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:58:26,583 - __main__ - INFO - Generation completed (iteration 3)\n",
      "2026-01-16 01:58:26,585 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-16 01:58:26,587 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-16 01:58:26,655 - __main__ - WARNING - PNG rendering failed. Fetching detailed syntax error...\n",
      "2026-01-16 01:58:26,675 - __main__ - ERROR - Syntax error detected: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:58:26,677 - __main__ - WARNING - Syntax error: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:58:26,679 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-16 01:58:26,679 - __main__ - INFO - Added syntax error feedback to generation prompt\n",
      "2026-01-16 01:58:48,421 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:58:48,444 - __main__ - INFO - Generation completed (iteration 4)\n",
      "2026-01-16 01:58:48,447 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-16 01:58:48,448 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-16 01:58:48,556 - __main__ - WARNING - PNG rendering failed. Fetching detailed syntax error...\n",
      "2026-01-16 01:58:48,578 - __main__ - ERROR - Syntax error detected: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:58:48,580 - __main__ - WARNING - Syntax error: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:58:48,581 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-16 01:58:48,582 - __main__ - INFO - Added syntax error feedback to generation prompt\n",
      "2026-01-16 01:59:12,076 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:59:12,097 - __main__ - INFO - Generation completed (iteration 5)\n",
      "2026-01-16 01:59:12,110 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-16 01:59:12,112 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-16 01:59:12,252 - __main__ - WARNING - PNG rendering failed. Fetching detailed syntax error...\n",
      "2026-01-16 01:59:12,264 - __main__ - ERROR - Syntax error detected: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:59:12,264 - __main__ - WARNING - Syntax error: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:59:12,266 - __main__ - INFO - --- NODE: GENERATE ---\n",
      "2026-01-16 01:59:12,268 - __main__ - INFO - Added syntax error feedback to generation prompt\n",
      "2026-01-16 01:59:34,800 - httpx - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-16 01:59:34,816 - __main__ - INFO - Generation completed (iteration 6)\n",
      "2026-01-16 01:59:34,824 - __main__ - INFO - --- NODE: SYNTAX_CHECK ---\n",
      "2026-01-16 01:59:34,825 - __main__ - INFO - Validating PlantUML syntax\n",
      "2026-01-16 01:59:34,936 - __main__ - WARNING - PNG rendering failed. Fetching detailed syntax error...\n",
      "2026-01-16 01:59:34,962 - __main__ - ERROR - Syntax error detected: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:59:34,965 - __main__ - WARNING - Syntax error: PlantUML Syntax Error:\n",
      "Unknown server error\n",
      "2026-01-16 01:59:34,968 - __main__ - WARNING - Max iterations (6) reached during syntax check\n",
      "2026-01-16 01:59:34,969 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:59:34,970 - __main__ - INFO - WORKFLOW COMPLETED\n",
      "2026-01-16 01:59:34,970 - __main__ - INFO - ============================================================\n",
      "2026-01-16 01:59:34,970 - __main__ - INFO - Iterations: 6\n",
      "2026-01-16 01:59:34,971 - __main__ - INFO - Syntax Valid: False\n",
      "2026-01-16 01:59:34,971 - __main__ - INFO - Logic Valid: False\n",
      "2026-01-16 01:59:34,975 - __main__ - INFO - PlantUML tool initialized with host: http://localhost:8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Iterations: 6\n",
      "Syntax Valid: False\n",
      "Logic Valid: False\n",
      "\n",
      "Diagram URL: http://localhost:8080/png/RP71QiCm38RlUGhJuw27lS0ePHGAsovz0YLM6rFR2hOqCDlUVRBJP5lPoRzawTDlUPSP4almtU0XPxCdkfgFpfZZQV-c1plsg2S8ZvHKJD9xbqTSzG3iA9g2K5Fm3iv3xxpZOfJDahkl6_iLGu-fqKEJUNIJEJvh726qATOcpBcYHufeejGo3HDUNEOqZxB0k49FB1OZDsg-wOQS4bqI14FdbYzhnm46yLn-kS4mUIg8SwF5ILSI5BIsl134LZcCE5n9UFQWjOABJCYIximTd3wYzfshW42YKBK6fU9_zhCyOlEuCTljvzEoAesxltR_grZaGxsQlQhksbVU9tSOhVxsDm==\n",
      "\n",
      "Generated Diagram:\n",
      "@startuml\n",
      "class Book {\n",
      "  ISBN\n",
      "  title\n",
      "  author\n",
      "  publisher\n",
      "  publication year\n",
      "  availability status\n",
      "}\n",
      "class Member {\n",
      "  membership ID\n",
      "  name\n",
      "  address\n",
      "  phone number\n",
      "  registration date\n",
      "}\n",
      "class Student {\n",
      "  student ID\n",
      "  program of study\n",
      "}\n",
      "class FacultyMember {\n",
      "  employee ID\n",
      "  department\n",
      "}\n",
      "class Loan {\n",
      "  checkout date\n",
      "  due date\n",
      "  return date\n",
      "}\n",
      "class Fine {\n",
      "  fine amount\n",
      "  payment status\n",
      "}\n",
      "Student \"1\" --|> Member \"1\"\n",
      "FacultyMember \"1\" --|> Member \"1\"\n",
      "Member \"*\" -- Loan \"*\"\n",
      "Book \"*\" -- Loan \"*\"\n",
      "@enduml\n"
     ]
    }
   ],
   "source": [
    "def run_single_test(\n",
    "    app: Any,\n",
    "    requirements: str,\n",
    "    exercise_name: str = \"Test Exercise\"\n",
    ") -> AgentState:\n",
    "    \"\"\"\n",
    "    Run the workflow on a single exercise.\n",
    "    \n",
    "    Args:\n",
    "        app: Compiled LangGraph workflow\n",
    "        requirements: Software requirements text\n",
    "        exercise_name: Name for logging purposes\n",
    "        \n",
    "    Returns:\n",
    "        Final workflow state\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"RUNNING: {exercise_name}\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Requirements preview: {requirements[:150]}...\")\n",
    "    \n",
    "    initial_state = create_initial_state(requirements)\n",
    "    \n",
    "    try:\n",
    "        final_output = app.invoke(initial_state, config={\"recursion_limit\": 50})\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"WORKFLOW COMPLETED\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Iterations: {final_output['iterations']}\")\n",
    "        logger.info(f\"Syntax Valid: {final_output['syntax_valid']}\")\n",
    "        logger.info(f\"Logic Valid: {final_output['logic_valid']}\")\n",
    "        \n",
    "        if final_output.get('best_diagram') and not final_output['logic_valid']:\n",
    "            if final_output['best_diagram'] != final_output['current_diagram']:\n",
    "                logger.info(\"Using BEST diagram instead of final (prevented regression)\")\n",
    "                final_output['current_diagram'] = final_output['best_diagram']\n",
    "        \n",
    "        return final_output\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Workflow execution failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Select and run a test exercise\n",
    "test_idx = 2\n",
    "requirements = test_exercises[test_idx][\"requirements\"]\n",
    "\n",
    "final_output = run_single_test(\n",
    "    app, \n",
    "    requirements, \n",
    "    f\"Exercise {test_idx + 1}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Iterations: {final_output['iterations']}\")\n",
    "print(f\"Syntax Valid: {final_output['syntax_valid']}\")\n",
    "print(f\"Logic Valid: {final_output['logic_valid']}\")\n",
    "\n",
    "if final_output['current_diagram']:\n",
    "    puml_tool = PlantUMLTool(config.plantuml_host)\n",
    "    diagram_url = puml_tool.get_diagram_url(final_output['current_diagram'])\n",
    "    print(f\"\\nDiagram URL: {diagram_url}\")\n",
    "    \n",
    "    print(\"\\nGenerated Diagram:\")\n",
    "    print(final_output['current_diagram']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "1e84c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@startuml\n",
      "class Book {\n",
      "  ISBN\n",
      "  title\n",
      "  author\n",
      "  publisher\n",
      "  publication year\n",
      "  availability status\n",
      "}\n",
      "class Member {\n",
      "  membership ID\n",
      "  name\n",
      "  address\n",
      "  phone number\n",
      "  registration date\n",
      "}\n",
      "class Student {\n",
      "  student ID\n",
      "  program of study\n",
      "}\n",
      "class FacultyMember {\n",
      "  employee ID\n",
      "  department\n",
      "}\n",
      "class Loan {\n",
      "  checkout date\n",
      "  due date\n",
      "  return date\n",
      "}\n",
      "class Fine {\n",
      "  fine amount\n",
      "  payment status\n",
      "}\n",
      "Student \"1\" --|> Member \"1\"\n",
      "FacultyMember \"1\" --|> Member \"1\"\n",
      "Member \"*\" -- Loan \"*\"\n",
      "Book \"*\" -- Loan \"*\"\n",
      "@enduml\n"
     ]
    }
   ],
   "source": [
    "print(final_output['current_diagram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "40178ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics(BaseModel):\n",
    "    \"\"\"Container for evaluation metrics.\"\"\"\n",
    "    precision: float = Field(ge=0.0, le=1.0, description=\"Precision score\")\n",
    "    recall: float = Field(ge=0.0, le=1.0, description=\"Recall score\")\n",
    "    f1: float = Field(ge=0.0, le=1.0, description=\"F1 score\")\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"P={self.precision:.2f}, R={self.recall:.2f}, F1={self.f1:.2f}\"\n",
    "\n",
    "\n",
    "class PlantUMLParser:\n",
    "    \"\"\"\n",
    "    Parser for extracting structured information from PlantUML diagrams.\n",
    "    \n",
    "    Extracts classes, attributes, and relationships from PlantUML code\n",
    "    for evaluation purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, plantuml_code: str):\n",
    "        \"\"\"\n",
    "        Initialize parser with PlantUML code.\n",
    "        \n",
    "        Args:\n",
    "            plantuml_code: PlantUML diagram code\n",
    "        \"\"\"\n",
    "        self.plantuml_code = plantuml_code\n",
    "        self.classes: Dict[str, Dict[str, List[str]]] = {}\n",
    "        self.relationships: List[Dict[str, Any]] = []\n",
    "        self.parse()\n",
    "    \n",
    "    def parse(self) -> None:\n",
    "        \"\"\"Parse the PlantUML code.\"\"\"\n",
    "        try:\n",
    "            self._extract_classes()\n",
    "            self._extract_relationships()\n",
    "            logger.debug(f\"Parsed {len(self.classes)} classes and {len(self.relationships)} relationships\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing failed: {e}\")\n",
    "    \n",
    "    def _extract_classes(self) -> None:\n",
    "        \"\"\"Extract class definitions and their attributes.\"\"\"\n",
    "        class_pattern = r'class\\s+(\\w+)\\s*\\{([^}]*)\\}'\n",
    "        matches = re.finditer(class_pattern, self.plantuml_code, re.MULTILINE | re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            class_name = match.group(1)\n",
    "            class_body = match.group(2)\n",
    "            \n",
    "            attributes = []\n",
    "            for line in class_body.strip().split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('--'):\n",
    "                    attributes.append(line)\n",
    "            \n",
    "            self.classes[class_name] = {'attributes': attributes}\n",
    "    \n",
    "    def _extract_relationships(self) -> None:\n",
    "        \"\"\"Extract relationships between classes with cardinalities.\"\"\"\n",
    "        patterns = [\n",
    "            # Generalization (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*<\\|--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'generalization'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--|>\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'generalization'),\n",
    "            \n",
    "            # Composition (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*\\*--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'composition'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--\\*\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'composition'),\n",
    "            \n",
    "            # Aggregation (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*o--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'aggregation'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--o\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'aggregation'),\n",
    "            \n",
    "            # Directed Association (either direction)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*-->\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'association'),\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*<--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'association'),\n",
    "            \n",
    "            # Simple Association (no arrow)\n",
    "            (r'(\\w+)\\s*(?:\"([^\"]*)\")?\\s*--\\s*(?:\"([^\"]*)\")?\\s*(\\w+)', 'association'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, rel_type in patterns:\n",
    "            for match in re.finditer(pattern, self.plantuml_code):\n",
    "                source = match.group(1)\n",
    "                target = match.group(4)\n",
    "                \n",
    "                # Skip if source or target is None or empty\n",
    "                if not source or not target:\n",
    "                    continue\n",
    "                \n",
    "                self.relationships.append({\n",
    "                    'type': rel_type,\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'cardinality_source': match.group(2) if match.lastindex >= 2 else None,\n",
    "                    'cardinality_target': match.group(3) if match.lastindex >= 3 else None\n",
    "                })\n",
    "\n",
    "\n",
    "class DiagramEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for comparing generated diagrams against gold standards.\n",
    "    \n",
    "    Computes precision, recall, and F1 scores for classes, attributes,\n",
    "    and relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gold_plantuml: str, pred_plantuml: str):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with gold and predicted diagrams.\n",
    "        \n",
    "        Args:\n",
    "            gold_plantuml: Gold standard PlantUML code\n",
    "            pred_plantuml: Predicted PlantUML code\n",
    "        \"\"\"\n",
    "        self.gold_parser = PlantUMLParser(gold_plantuml)\n",
    "        self.pred_parser = PlantUMLParser(pred_plantuml)\n",
    "    \n",
    "    def _normalize_attr(self, attr_str: str) -> str:\n",
    "        \"\"\"Normalize attribute strings for comparison.\"\"\"\n",
    "        return attr_str.split(':')[0].strip().lower()\n",
    "    \n",
    "    def _normalize_rel_type(self, rel_type: str) -> str:\n",
    "        \"\"\"Normalize relationship types.\"\"\"\n",
    "        mapping = {\n",
    "            '<|--': 'INHERITANCE',\n",
    "            '--|>': 'INHERITANCE',\n",
    "            '*--': 'COMPOSITION',\n",
    "            '--*': 'COMPOSITION',\n",
    "            'o--': 'AGGREGATION',\n",
    "            '--o': 'AGGREGATION',\n",
    "            '--': 'ASSOCIATION',\n",
    "            '<--': 'ASSOCIATION',\n",
    "            '-->': 'ASSOCIATION'\n",
    "        }\n",
    "        return mapping.get(rel_type, 'ASSOCIATION')\n",
    "    \n",
    "    def _calculate_metrics(\n",
    "        self, \n",
    "        gold_set: set, \n",
    "        pred_set: set\n",
    "    ) -> EvaluationMetrics:\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1 scores.\n",
    "        \n",
    "        Args:\n",
    "            gold_set: Set of gold standard elements\n",
    "            pred_set: Set of predicted elements\n",
    "            \n",
    "        Returns:\n",
    "            EvaluationMetrics object\n",
    "        \"\"\"\n",
    "        tp = len(gold_set.intersection(pred_set))\n",
    "        fp = len(pred_set - gold_set)\n",
    "        fn = len(gold_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return EvaluationMetrics(\n",
    "            precision=round(precision, 2),\n",
    "            recall=round(recall, 2),\n",
    "            f1=round(f1, 2)\n",
    "        )\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, EvaluationMetrics]:\n",
    "        \"\"\"\n",
    "        Get all evaluation metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with metrics for classes, attributes, and relationships\n",
    "        \"\"\"\n",
    "        # Classes \n",
    "        gold_classes = {c.lower() for c in self.gold_parser.classes.keys()}\n",
    "        pred_classes = {c.lower() for c in self.pred_parser.classes.keys()}\n",
    "        \n",
    "        # Attributes \n",
    "        gold_attrs = set()\n",
    "        for cls, info in self.gold_parser.classes.items():\n",
    "            for attr in info['attributes']:\n",
    "                gold_attrs.add((cls.lower(), self._normalize_attr(attr)))\n",
    "        \n",
    "        pred_attrs = set()\n",
    "        for cls, info in self.pred_parser.classes.items():\n",
    "            for attr in info['attributes']:\n",
    "                pred_attrs.add((cls.lower(), self._normalize_attr(attr)))\n",
    "        \n",
    "        # Relationships (type + direction only)\n",
    "        gold_rels = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']))\n",
    "            for r in self.gold_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        pred_rels = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']))\n",
    "            for r in self.pred_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        \n",
    "        # Cardinalities \n",
    "        gold_rels_card = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']),\n",
    "             (r['cardinality_source'] or '').strip(), (r['cardinality_target'] or '').strip())\n",
    "            for r in self.gold_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        pred_rels_card = {\n",
    "            (r['source'].lower(), r['target'].lower(), self._normalize_rel_type(r['type']),\n",
    "             (r['cardinality_source'] or '').strip(), (r['cardinality_target'] or '').strip())\n",
    "            for r in self.pred_parser.relationships\n",
    "            if r.get('source') and r.get('target')\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"classes\": self._calculate_metrics(gold_classes, pred_classes),\n",
    "            \"attributes\": self._calculate_metrics(gold_attrs, pred_attrs),\n",
    "            \"relationships\": self._calculate_metrics(gold_rels, pred_rels),\n",
    "            \"cardinalities\": self._calculate_metrics(gold_rels_card, pred_rels_card)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def evaluate_diagram(\n",
    "    gold_standard: str,\n",
    "    generated_diagram: str\n",
    ") -> Dict[str, EvaluationMetrics]:\n",
    "    \"\"\"\n",
    "    Evaluate a generated diagram against gold standard.\n",
    "    \n",
    "    Args:\n",
    "        gold_standard: Gold standard PlantUML code\n",
    "        generated_diagram: Generated PlantUML code\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    evaluator = DiagramEvaluator(gold_standard, generated_diagram)\n",
    "    return evaluator.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "389c5565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION METRICS\n",
      "============================================================\n",
      "\n",
      "Classes:       P=0.83, R=0.71, F1=0.77\n",
      "Attributes:    P=0.30, R=0.27, F1=0.29\n",
      "Relationships: P=1.00, R=0.29, F1=0.44\n",
      "Cardinalities: P=0.00, R=0.00, F1=0.00\n",
      "\n",
      "============================================================\n",
      "OVERALL F1 SCORE: 0.53\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "gold_standard = test_exercises[test_idx][\"solution_plantuml\"]\n",
    "generated_diagram = final_output[\"current_diagram\"]\n",
    "\n",
    "metrics = evaluate_diagram(gold_standard, generated_diagram)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nClasses:       {metrics['classes']}\")\n",
    "print(f\"Attributes:    {metrics['attributes']}\")\n",
    "print(f\"Relationships: {metrics['relationships']}\")\n",
    "print(f\"Cardinalities: {metrics['cardinalities']}\")\n",
    "\n",
    "weighted_avg_f1 = (\n",
    "    metrics['classes'].f1 * 0.4 + \n",
    "    metrics['attributes'].f1 * 0.3 + \n",
    "    metrics['relationships'].f1 * 0.3\n",
    "    # metrics['cardinalities'].f1 * 0.2\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OVERALL F1 SCORE: {weighted_avg_f1:.2f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
